<!-- This layout is used in all pages. Making changes here will efect all pages. We recommend not to change anything here. --> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /><link rel="dns-prefetch" href="//fonts.googleapis.com" /><link rel="dns-prefetch" href="//google-analytics.com" /><link rel="dns-prefetch" href="//www.google-analytics.com" /><link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com" /><link rel="dns-prefetch" href="//ajax.googleapis.com" /><link rel="dns-prefetch" href="//fonts.gstatic.com" /><link rel="dns-prefetch" href="https://webjeda-demo.disqus.com/" /><title>Expectation-Maximization for News Clustering | Welcome to my Website</title><meta name="generator" content="Jekyll v3.8.4" /><meta property="og:title" content="Expectation-Maximization for News Clustering" /><meta name="author" content="Alan Gewerc" /><meta property="og:locale" content="en_US" /><meta name="description" content="Image source: Sklearn" /><meta property="og:description" content="Image source: Sklearn" /><link rel="canonical" href="http://localhost:4000/blog/EM-Clustering/" /><meta property="og:url" content="http://localhost:4000/blog/EM-Clustering/" /><meta property="og:site_name" content="Welcome to my Website" /><meta property="og:image" content="http://localhost:4000/assets/images/cluster.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2018-11-01T00:00:00+08:00" /><script type="application/ld+json"> {"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/EM-Clustering/"},"datePublished":"2018-11-01T00:00:00+08:00","description":"Image source: Sklearn","image":"http://localhost:4000/assets/images/cluster.png","author":{"@type":"Person","name":"Alan Gewerc"},"headline":"Expectation-Maximization for News Clustering","url":"http://localhost:4000/blog/EM-Clustering/","dateModified":"2018-11-01T00:00:00+08:00","@context":"https://schema.org"}</script><link rel="stylesheet" href="/assets/css/main-default.css" /><link id="color-scheme" rel="stylesheet" href="/assets/css/main-default.css" /><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/apple-icon-60x60.png" /><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/apple-icon-114x114.png" /><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/apple-icon-152x152.png" /><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/android-icon-192x192.png" /><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" /><link rel="icon" href="/favicon.ico" type="image/x-icon" /><script src="https://cdn.jsdelivr.net/npm/ga-lite@1/dist/ga-lite.min.js" async></script><script> var galite = galite || {}; galite.UA = 'UA-174262091-1';</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script></head><div class="wrapper"><div class="container"><div class="main"><div class="main-container shadow"><div class="title-space"> <h1>Expectation-Maximization for News Clustering</h1></div><hr class="dashed"> <main> <ul class="breadcrumbs"> <li><a href="/">Home</a></li> <li><a href="/blog/">Blog</a></li> <li><a href="#">Em clustering</a></li> </ul><div class="meta" data-aos="fade-up"> <p> <small> <span> <i class="fa fa-calendar" aria-hidden="true"></i> 01 Nov 2018&nbsp; </span> <span> <i class="fa fa-user" aria-hidden="true"></i> Alan Gewerc&nbsp; </span> <span> <i class="fa fa-clock-o" aria-hidden="true"></i> 63 mins read. </span> </small> </p></div><div class="featured-image" style="background-image: url(/assets/images/cluster.png)" data-aos="zoom-in" ></div><div class="container"> <article> <p><em>Image source: Sklearn</em></p> <p><br /></p> <h3 id="introduction-to-clustering">Introduction to Clustering</h3> <p>Data clustering is the most common form of <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a>. The primary goal of unsupervised learning is to detect patterns in unlabeled data, which means there are no previous targets to forecasted. Clustering algorithms are useful for several tasks such as anomaly detection, customer segmentation, Insurance Fraud Detection and so on. Some of the most popular clustering algorithms developed are <a href="https://epubs.siam.org/doi/abs/10.1137/1.9780898718348.ch9?mobileUi=0">Center-based</a> partitioning (e.g., KMeans), <a href="https://en.wikipedia.org/wiki/Cluster_analysis#Density-based_clustering">density-based</a> clustering (e.g., DBSCAN), <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical</a> clustering, and <a href="http://www.slideshare.net/ssakpi/graph-based-clustering">graphbased</a> clustering. <br /></p> <p>From <a href="https://www.geeksforgeeks.org/clustering-in-machine-learning/">geeksforgeeks</a> <br /> <em>Clustering is very important as it determines the intrinsic grouping among the unlabeled data present. There are no predefined rules to judge what is good clustering. It depends on the user, what is the criteria they may use which satisfy their need. For instance, we could be interested in finding representatives for homogeneous groups (data reduction), in finding “natural clusters” and describe their unknown properties (“natural” data types), in finding useful and suitable groupings (“useful” data classes) or in finding unusual data objects (outlier detection). This algorithm must make some assumptions which constitute the similarity of points, and each assumption make different and equally valid clusters.”</em></p> <p>Here is an intuitive example of how clustering works:</p> <p><img style="float: left;" src="/assets/images/simpsons.jpg" alt="drawing" width="600" height="500" /></p> <p><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p> <p>From this image, we can understand how clustering works. We will choose different attributes from our data points. In this image family, place and gender are used to create clusters. Other possibilities could be considered. We will always make numerical manipulations of our points according to the features we have from the data. <br /></p> <h3 id="expectation-maximization-for-clustering-">Expectation Maximization for Clustering <br /></h3> <p>The approach we will follow for EM in this project follows the work developed by professor <a href="http://users.monash.edu.au/~gholamrh/">Gholamreza Haffari</a> from Monash University.</p> <p>In this project, we will cluster news from the BBC dataset. There are 2225 articles; each labelled under one of 5 categories: business, entertainment, politics, sport or tech. However, we are not interested in the labels as we will be applying an unsupervised technique. We will use the Expectation-Maximization algorithm.</p> <p>From <a href="https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/expectation_maximization_clustering.html">rapidminer</a><br /> <i>The EM (expectation-maximization) technique is similar to the K-Means technique. The basic operation of K-Means clustering algorithms is relatively simple: Given a fixed number of k clusters, assign observations to those clusters so that the means across clusters (for all variables) are as different from each other as possible. The EM algorithm extends this basic approach to clustering in two important ways: <br /></i></p> <ul> <li>Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters. <br /></li> <li>The basic approach and logic of this clustering method are as follows. Suppose you measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. The goal of EM clustering is to estimate the means and standard deviations for each cluster to maximize the likelihood of the observed data (distribution). Put another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. <br /></li> <li>The results of EM clustering are different from those computed by k-means clustering. The latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. In other words, each observation belongs to each cluster with a certain probability. Of course, as a final result, you can usually review an actual assignment of observations to clusters, based on the (largest) classification probability.</li> </ul> <p><em>An image from the Ardian Umam</em> <a href="https://ardianumam.wordpress.com/2017/11/07/how-em-expectation-maximization-method-works-for-clustering/">blog</a><br /> <img src="/assets/images/em2.png" alt="drawing" width="500" height="300" /> <br /><br /><br /></p> <ul> <li><em>This is 3D Visualization of different clusters.</em></li> <li><em>As we can see they follow a gaussian distribution.</em></li> <li><em>We want to maximize the likelihood of points belonging to clusters .</em></li> </ul> <p><br /></p> <h2 id="project-development">Project Development</h2> <p>Programming Language: R 3.6.1 in Jupyter Notebook</p> <p>Libraries:</p> <ul> <li>reshape2</li> <li>ggplot2</li> <li>repr</li> <li>NLP</li> <li>tm</li> </ul> <h4 id="import-libraries">Import Libraries</h4><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">warn</span><span class="o">=</span><span class="m">-1</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">reshape2</span><span class="p">)</span><span class="w"> </span><span class="c1"># for melt and cast functions</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w"> </span><span class="c1"># for plotting functions</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">repr</span><span class="p">)</span><span class="w"> </span><span class="c1"># to resize the plots</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">NLP</span><span class="p">)</span><span class="w"> </span><span class="c1"># natural language preprocessing</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span><span class="w"> </span><span class="c1"># text mining library</span><span class="w">
</span></code></pre></div></div><p><br /><br /></p> <h2 id="expectation-maximization-math-for-document-clustering">Expectation Maximization Math for Document Clustering<br /></h2> <p>We first derive the Expectation and Maximization steps of the <strong>hard-EM</strong> algorithm for Document Clustering:</p> <p>In <strong>Expectation</strong> and <strong>Maximization</strong> steps we have incomplete data, i.e., the documents clusters are not given to us so the latent variables<script type="math/tex">{z_1, z_2, ... , z_N}</script>are unseen.</p> <p>Let’s be<script type="math/tex">theta := (\varphi,\mu_1,...,\mu_K)</script>the collection of parameters where the parameters means:</p> <ul> <li><script type="math/tex">\varphi=(\varphi_1,\varphi_2,...,\varphi_K)</script>is the clusters proportion, with<script type="math/tex">\varphi_k \ge 0</script>and<script type="math/tex">\sum_{k=1}^{K} \varphi_k =1</script>(1)</li> <li><script type="math/tex">\mu_k=(\mu_{k,1},\mu_{k,2},...,\mu_{k,\|\mathcal{A}\| })</script>is the word proportion for each cluster, with<script type="math/tex">\mu_{k,w} \ge 0</script>and<script type="math/tex">\sum_{w \in \mathcal{A}} \mu_{k,w} =1</script>(2)</li> </ul> <p>Our likelihood, the probability of the observed documents is:<script type="math/tex">p(d_1,...d_N)=\prod _{n=1}^{N}p(d_n)=\prod _{n=1}^{N}\sum _{k=1}^{K}p(z_{n,k}=1,d_n)=\prod _{n=1}^{N}\sum _{k=1}^{K}\left ( \varphi _{k}\prod _{w\in\mathcal{A}}\mu_{k,w}^{c(w,d_n)} \right )</script></p> <p>Applying the log in the equation above it yields:<script type="math/tex">ln\ p(d_1,...d_N)=\sum _{n=1}^{N}ln\ p(d_n)=\sum _{n=1}^{N}ln\sum _{k=1}^{K}p(z_{n,k}=1,d_n)=\sum _{n=1}^{N}ln\sum _{k=1}^{K}\left ( \varphi _{k}\prod _{w\in\mathcal{A}}\mu_{k,w}^{c(w,d_n)} \right )</script></p> <p>And the<script type="math/tex">\mathcal{Q}</script>function (that is the basis of the EM algorithm) takes the form of:<script type="math/tex">\mathcal{Q}\left ( \theta,\theta^{old} \right ):=\sum _{n=1}^{N}\sum _{k=1}^{K}p(z_{n,k}=1|d_n,\theta^{old})\ ln\ p(z_{n,k}=1,d_n|\theta)\\ \\=\sum _{n=1}^{N}\sum _{k=1}^{K}p(z_{n,k}=1|d_n,\theta^{old})\left ( ln\varphi _k + \sum _{w\in\mathcal{A}}{c(w,d_n)}\ ln\ \mu_{k,w}\right )\\ =\sum _{n=1}^{N}\sum _{k=1}^{K}\gamma (z_{n,k})\left ( ln\varphi _k + \sum _{w\in\mathcal{A}}{c(w,d_n)}\ ln\ \mu_{k,w}\right )\\</script></p> <p>where<script type="math/tex">\gamma (z_{n,k}):= p(z_{n,k}=1 \| d_n,\theta^{old})</script>are the responsability factors.</p> <p>Maximizing the<script type="math/tex">\mathcal{Q}</script>function using the Lagrangian to enforce the constraints (1) and (2) above, and setting the derivatives to zero leads to the following solutions for the parameters:</p> <ul> <li><script type="math/tex">\varphi_k=\frac{N _k}{N}</script>where<script type="math/tex">N_k := \sum _{n=1}^{N}\gamma (z_{n,k})</script>(3) being the cluster proportion</li> <li><script type="math/tex">\mu_{k,w}=\frac{\sum _{n=1}^{N}\gamma (z_{n,k})c(w,d_n)}{\sum _{w'\in\mathcal{A}} \sum _{n=1}^{N}\gamma (z_{n,k})c(w',d_n)}</script>(4) being the word proportion for each cluster</li> </ul> <p>Thus, the EM Algorithm to learn the parameters and find the best values for the latent variables will follow these steps:</p> <p>1) Choose an initial setting for the parameters<script type="math/tex">\theta^{old} := (\varphi^{old},\mu_1^{old},...,\mu_K^{old})</script></p> <p>In the case of <strong>hard-EM algorithm</strong>, each data is assigned to the class with the largest posterior probability. Thus:<br /><script type="math/tex">Z^{*} = argmax_z\ \gamma (z_{n,k})= argmax_z\ p(z_{n,k}=1|d_n,\theta^{old})</script></p> <p>And there is no expectation in over the latent variables in the definition of the<script type="math/tex">\mathcal{Q}</script>function. Thus:<script type="math/tex">\mathcal{Q}(\theta,\theta^{old})= \sum _{n=1}^{N} ln\ p(z_{n,k=Z^*}=1,d_n|\theta)</script></p> <p>2) While the convergence (stop condition) is not met:</p> <ul> <li><strong>Expectation (E) step:</strong> based on the current values for the parameters<script type="math/tex">\theta^{old} := (\varphi^{old},\mu_1^{old},...,\mu_K^{old})</script>we will set<script type="math/tex">\forall n</script>and<script type="math/tex">\forall k</script>a<script type="math/tex">Z^*</script>such as:</li> </ul><script type="math/tex; mode=display">Z^{*} \leftarrow argmax_z\ \gamma (z_{n,k})= argmax_z\ p(z_{n,k}=1|d_n,\theta^{old})</script><ul> <li><strong>Maximization (M) step:</strong> based on the result of<script type="math/tex">Z^{*}</script>calculated on the <strong>E-step</strong> above, we re-estimate the values of parameters<script type="math/tex">(\varphi_k,\mu_{k,w})</script>to calculate the<script type="math/tex">\theta^{new}</script>using the equations (3) and (4) above:</li> </ul><script type="math/tex; mode=display">\theta^{new} \leftarrow argmax_\theta\ \mathcal{Q}(\theta,\theta^{old})= argmax_\theta \sum _{n=1}^{N} ln\ p(z_{n,k=Z^*}^{*}=1,d_n|\theta)\\ = argmax_\theta\ \sum _{n=1}^{N}\left (ln\varphi _{k=Z^*} + \sum _{w\in\mathcal{A}}{c(w,d_n)}ln\ \mu_{k=Z^*,w}\right )</script><p>what setting the partial derivatives to zero leads to the following solutions for the parameters:</p> <ul> <li><script type="math/tex">\varphi_k^{new}=\frac{N _k}{N}</script>where<script type="math/tex">N_k := \sum _{n=1}^{N}z_{n,k=Z^*}</script>–&gt; being the cluster proportion</li> <li><script type="math/tex">\mu_{k,w}^{new}=\frac{\sum _{n=1}^{N}z_{n,k=Z^*}c(w,d_n)}{\sum _{w'\in\mathcal{A}} \sum _{n=1}^{N} z_{n,k=Z^*}c(w',d_n)}</script>–&gt; being the word proportion for each cluster</li> </ul> <p>3)<script type="math/tex">\theta^{old}\leftarrow \theta^{new}</script></p> <p>Implementation of the Hard-EM (derived above) and Soft-EM for Document Clustering.</p> <h4 id="helper-function">Helper Function</h4> <p>This function is needed to prevent numerical overflow/underflow when working with small numbers, because we can easily get small numbers by multiplying<script type="math/tex">% <![CDATA[ p1 * p2 * ... * pn (where 0 <= pi <= 1$ are probabilities) %]]></script>. Example: Suppose we are interested in<script type="math/tex">p1xp2xp3 + q1xq2xq3</script>where all numbers are probabilities in<script type="math/tex">[0,1]</script>. To prevent numerical errors, we do the computation in the log space and convert the result back using the exp func. Hence our approach is to form the vector<script type="math/tex">v = log(p1)+log(p2)+log(p3) , log(q1)+log(q2)+log(q3)]</script>Then get the results by:<script type="math/tex">exp(logSum(v))</script><br /> Input:<script type="math/tex">logA1, logA2 ... logAn</script><br /> Output:<script type="math/tex">log(A1+A2+...+An)</script></p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logSum</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
   </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w">
   </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">m</span><span class="p">))))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="initialize-model-parameters-randomly">Initialize model parameters randomly</h3><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial.param</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">123456</span><span class="p">){</span><span class="w">
  </span><span class="n">set.seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span><span class="w">
  </span><span class="n">rho</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="n">K</span><span class="p">,</span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="c1"># assume all clusters have the same size (we will update this later on)</span><span class="w">
  </span><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">runif</span><span class="p">(</span><span class="n">K</span><span class="o">*</span><span class="n">vocab_size</span><span class="p">),</span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vocab_size</span><span class="p">)</span><span class="w">    </span><span class="c1"># initiate Mu </span><span class="w">
  </span><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prop.table</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">margin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">   </span><span class="c1"># normalization to ensure that sum of each row is 1</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"rho"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rho</span><span class="p">,</span><span class="w"> </span><span class="s2">"mu"</span><span class="o">=</span><span class="w"> </span><span class="n">mu</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="e-step-for-document-clustering">E-Step for Document Clustering</h3> <p>We now use a function to perform the E-step iteration. Some information about the parameters of the function:</p> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">gamma</code>: the matrix of posterior probabilities NxK</li> <li><code class="highlighter-rouge">model</code>: the list of parameters (priors Kx1 and means KxVocabSize)</li> <li><code class="highlighter-rouge">counts</code>: the word-document frequency matrix</li> <li><code class="highlighter-rouge">soft</code>: TRUE if soft EM, FALSE if Hard EM</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">gamma</code>: the matrix of posterior probabilities NxK for the given parameters (if soft = TRUE)</li> <li><code class="highlighter-rouge">z_star</code>: the vector Nx1 of hard choosen classes for each document given gamma (if soft = FALSE)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">E.step</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
  </span><span class="c1"># Model Parameter Setting</span><span class="w">
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">

  </span><span class="c1"># E step:    </span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">){</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
      </span><span class="c1">## calculate the posterior based on the estimated mu and rho in the "log space"</span><span class="w">
      </span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="m">1</span><span class="p">])</span><span class="w"> </span><span class="o">+</span><span class="w">  </span><span class="nf">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]))</span><span class="w"> 
    </span><span class="p">}</span><span class="w">
    </span><span class="c1"># normalisation to sum to 1 in the log space</span><span class="w">
    </span><span class="n">logZ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">logSum</span><span class="p">(</span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,])</span><span class="w">
    </span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">logZ</span><span class="w">
  </span><span class="p">}</span><span class="w">
  
  </span><span class="c1"># converting back from the log space </span><span class="w">
  </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span><span class="w">
  </span><span class="c1"># if it is hard EM, we need to select the K with highest gamma</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">){</span><span class="w">
      </span><span class="n">z_star</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max.col</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="s1">'first'</span><span class="p">)</span><span class="w"> </span><span class="c1"># gets the "argmax" class for each observation   </span><span class="w">
  </span><span class="p">}</span><span class="w">
  </span><span class="c1"># if we are doing Hard-EM, we return "z_star", which are the classes assigned for each n, </span><span class="w">
  </span><span class="c1"># if we are doing soft-EM, we return "gamma", which is the matrix with posterior probabilities for each k,n</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="o">==</span><span class="kc">FALSE</span><span class="p">){</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="n">z_star</span><span class="p">)</span><span class="w">
  </span><span class="p">}</span><span class="k">else</span><span class="p">{</span><span class="nf">return</span><span class="p">(</span><span class="n">gamma</span><span class="p">)}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="m-step-for-document-clustering">M Step for Document Clustering</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">gamma.z_star</code> (if soft = TRUE) : the matrix of posterior probabilities NxK for the given parameters</li> <li><code class="highlighter-rouge">gamma.z_star</code> (if soft = FALSE) : the vector Nx1 of hard choosen classes for each document given gamma</li> <li><code class="highlighter-rouge">model</code>: the list of old parameters (class priors Kx1 and class means KxVocabSize)</li> <li><code class="highlighter-rouge">counts</code>: the word-document frequency matrix</li> <li><code class="highlighter-rouge">soft</code>: TRUE if soft EM, FALSE if Hard EM</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">model</code>: the list of updated parameters (priors Kx1 and means KxVocabSize)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">M.step</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">gamma.z_star</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
  </span><span class="c1"># Model Parameter Setting</span><span class="w">
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w">   </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">W</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">   </span><span class="c1"># number of words i.e. vocabulary size</span><span class="w">
  </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of clusters</span><span class="w">
  </span><span class="c1"># to avoid NaN where all elements are zeros when calculating the new  means</span><span class="w">
  </span><span class="n">eps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">1e-10</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w">
  
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="o">==</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w"> </span><span class="c1"># Soft-EM</span><span class="w">
      </span><span class="n">gamma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma.z_star</span><span class="w">
      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
          </span><span class="c1">## recalculate the estimations:</span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="n">N</span><span class="w">  </span><span class="c1"># the relative cluster size</span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">((</span><span class="n">counts</span><span class="o">%*%</span><span class="n">gamma</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="nf">sum</span><span class="p">((</span><span class="n">counts</span><span class="o">%*%</span><span class="n">gamma</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="w"> </span><span class="c1"># new means</span><span class="w">
    
    </span><span class="p">}</span><span class="w">      
  </span><span class="p">}</span><span class="k">else</span><span class="p">{</span><span class="w"> </span><span class="c1"># Hard-EM</span><span class="w">
      </span><span class="n">z_star</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma.z_star</span><span class="w">
      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
          </span><span class="c1">## recalculate the estimations:</span><span class="w">
          </span><span class="c1">## recalculate the estimations:</span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="w">  </span><span class="c1"># the relative cluster size          </span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rowSums</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">rowSums</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">]))</span><span class="w"> 
          </span><span class="c1"># new means</span><span class="w">
    </span><span class="p">}</span><span class="w">

  </span><span class="p">}</span><span class="w">

  </span><span class="c1"># Return the result</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="the-training-objective-function">The Training Objective Function</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">model</code>: the model object containing the mu and rho</li> <li><code class="highlighter-rouge">counts</code>: the word-document frequency matrix</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">nloglike</code>: the negative log-likelihood i.e. log P(counts / model)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> 
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">
   
  </span><span class="n">nloglike</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">){</span><span class="w">
    </span><span class="n">lprob</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="o">=</span><span class="n">K</span><span class="p">)</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
      </span><span class="n">lprob</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]))</span><span class="w"> 
    </span><span class="p">}</span><span class="w">
    </span><span class="n">nloglike</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nloglike</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">logSum</span><span class="p">(</span><span class="n">lprob</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">))</span><span class="w">
  </span><span class="p">}</span><span class="w">
  
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">nloglike</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="em-for-document-clustering">EM for Document Clustering</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">counts</code>: word count matrix</li> <li><code class="highlighter-rouge">K</code>: the number of clusters</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">model</code>: a list of model parameters</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EM</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">max.epoch</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">123456</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
  
  </span><span class="c1"># Model Parameter Setting</span><span class="w">
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">W</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of unique words (in all documents)</span><span class="w">
  
  </span><span class="c1"># Initialization</span><span class="w">
  </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">initial.param</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span><span class="w">
  </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">)</span><span class="w">

  </span><span class="n">print</span><span class="p">(</span><span class="n">train_obj</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">counts</span><span class="p">))</span><span class="w">
  </span><span class="c1"># Build the model</span><span class="w">
  </span><span class="k">for</span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">max.epoch</span><span class="p">){</span><span class="w">
    
    </span><span class="c1"># E Step</span><span class="w">
    </span><span class="n">gamma_kmax</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">E.step</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">soft</span><span class="p">)</span><span class="w">
    </span><span class="c1"># M Step</span><span class="w">
    </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">M.step</span><span class="p">(</span><span class="n">gamma_kmax</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">soft</span><span class="p">)</span><span class="w">
   
    </span><span class="n">print</span><span class="p">(</span><span class="n">train_obj</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">))</span><span class="w"> 
  </span><span class="p">}</span><span class="w">
  </span><span class="c1"># Return Model</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="o">==</span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"model"</span><span class="o">=</span><span class="n">model</span><span class="p">,</span><span class="s2">"gamma"</span><span class="o">=</span><span class="n">gamma_kmax</span><span class="p">))}</span><span class="k">else</span><span class="p">{</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"model"</span><span class="o">=</span><span class="n">model</span><span class="p">,</span><span class="s2">"k_max"</span><span class="o">=</span><span class="n">gamma_kmax</span><span class="p">))</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="read-and-pre-process-data">Read and Pre-process Data</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">file.name</code>: name of the input .txt file</li> <li><code class="highlighter-rouge">spr.ratio</code>: is used to reduce the sparsity of data by removing very infrequent words</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">docs</code>: the unlabeled corpus (each row is a document)</li> <li><code class="highlighter-rouge">word.doc.mat</code>: the count matrix (rows and columns corresponds to words and documents, respectively)</li> <li><code class="highlighter-rouge">label</code>: the real cluster labels (will be used in visualization/validation and not for clustering)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reading the data</span><span class="w">
</span><span class="n">read.data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">file.name</span><span class="o">=</span><span class="s1">'./bbc-text.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">spr.ratio</span><span class="o">=</span><span class="m">0.90</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">

  </span><span class="c1"># Read the data</span><span class="w">
  </span><span class="n">text_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="n">file.name</span><span class="p">,</span><span class="w"> </span><span class="n">colClasses</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'factor'</span><span class="p">,</span><span class="w"> </span><span class="s1">'character'</span><span class="p">))</span><span class="w">
  </span><span class="c1">## the terms before the first '\t' are the lables (the newsgroup names) and all the remaining text after '\t' </span><span class="w">
  </span><span class="c1">##are the actual documents</span><span class="w">
  </span><span class="n">docs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">text_df</span><span class="w">
  </span><span class="n">colnames</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"doc_id"</span><span class="p">,</span><span class="w"> </span><span class="s2">"text"</span><span class="p">)</span><span class="w">
  </span><span class="n">docs</span><span class="o">$</span><span class="n">doc_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rownames</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w">
  </span><span class="c1"># store the labels for evaluation</span><span class="w">
  </span><span class="n">labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">text_df</span><span class="o">$</span><span class="n">category</span><span class="w">
    
  </span><span class="n">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span><span class="w">
  </span><span class="c1"># create a corpus</span><span class="w">
  </span><span class="n">docs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">DataframeSource</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Corpus</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># Preprocessing:</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">removeWords</span><span class="p">,</span><span class="w"> </span><span class="n">stopwords</span><span class="p">(</span><span class="s2">"english"</span><span class="p">))</span><span class="w"> </span><span class="c1"># remove stop words </span><span class="w">
    </span><span class="c1">#(the most common word in a language that can be find in any document)</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">removePunctuation</span><span class="p">)</span><span class="w"> </span><span class="c1"># remove pnctuation</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">stemDocument</span><span class="p">)</span><span class="w"> </span><span class="c1"># perform stemming (reducing inflected and derived words to their root form)</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">removeNumbers</span><span class="p">)</span><span class="w"> </span><span class="c1"># remove all numbers</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">stripWhitespace</span><span class="p">)</span><span class="w"> </span><span class="c1"># remove redundant spaces </span><span class="w">
  </span><span class="c1"># Create a matrix which its rows are the documents and colomns are the words. </span><span class="w">
  </span><span class="n">dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">DocumentTermMatrix</span><span class="p">(</span><span class="n">corp</span><span class="p">)</span><span class="w">
  </span><span class="c1">## reduce the sparcity of out dtm</span><span class="w">
  </span><span class="n">dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">removeSparseTerms</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span><span class="w"> </span><span class="n">spr.ratio</span><span class="p">)</span><span class="w">
  </span><span class="c1">## convert dtm to a matrix</span><span class="w">
  </span><span class="n">word.doc.mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">dtm</span><span class="p">))</span><span class="w">
  
  </span><span class="c1"># Return the result</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"docs"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">docs</span><span class="p">,</span><span class="w"> </span><span class="s2">"word.doc.mat"</span><span class="o">=</span><span class="w"> </span><span class="n">word.doc.mat</span><span class="p">,</span><span class="w"> </span><span class="s2">"labels"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">labels</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1"># Reading documents </span><span class="w">
</span><span class="c1">## Note: sample.size=0 means all read all documents!</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.data</span><span class="p">(</span><span class="n">file.name</span><span class="o">=</span><span class="s1">'./bbc-text.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">spr.ratio</span><span class="o">=</span><span class="w"> </span><span class="m">.99</span><span class="p">)</span><span class="w">
</span><span class="c1"># word-document frequency matrix </span><span class="w">
</span><span class="n">counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">word.doc.mat</span><span class="w">
</span></code></pre></div></div><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><table> <thead> <tr> <th style="text-align: center"><strong>Word</strong></th> <th style="text-align: center"><strong>Doc1</strong></th> <th style="text-align: center"><strong>Doc2</strong></th> <th style="text-align: center"><strong>Doc3</strong></th> <th style="text-align: center"><strong>Doc4</strong></th> <th style="text-align: center"><strong>…</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">accord</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">adam</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">advert</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">advertis</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">allow</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">alreadi</td> <td style="text-align: center">1</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">1</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">also</td> <td style="text-align: center">3</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">although</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">announc</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">annual</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">anybodi</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">avail</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">bbc</td> <td style="text-align: center">3</td> <td style="text-align: center">0</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">big</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">biggest</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">bill</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">book</td> <td style="text-align: center">1</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">box</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">brand</td> <td style="text-align: center">7</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">…</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> </tbody> </table><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#  run soft-EM algorithm on the provided data</span><span class="w">
</span><span class="n">res_soft</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">EM</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">max.epoch</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200</span><span class="p">)</span><span class="w">  
</span><span class="c1"># visualization</span><span class="w">
</span><span class="c1">## find the cluster with the maximum probability (since we have soft assignment here)</span><span class="w">
</span><span class="n">label.hat.soft</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">res_soft</span><span class="o">$</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">which.max</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#  run soft-EM algorithm on the provided data</span><span class="w">
</span><span class="n">res_hard</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">EM</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">max.epoch</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200</span><span class="p">)</span><span class="w">  
</span><span class="c1"># visualization</span><span class="w">
</span><span class="c1">## find the choosen cluster (since we have hard assignment here)</span><span class="w">
</span><span class="n">label.hat.hard</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">res_hard</span><span class="o">$</span><span class="n">k_max</span><span class="w">
</span></code></pre></div></div><h1 id="visualizing-clusters-with-pca">Visualizing Clusters With PCA</h1> <p>We want to visualise the clusters, but we have 2347 variables. It’s not possible to plot it directly; we need to reduce the number of dimensions, ideally to 2 or 3 so that we can get a good broad view of the patterns captured. A popular and simple technique for dimensionality reduction is <strong>PCA</strong>. I won’t derive the calculation of the principal components here, but, in a nutshell, it provides variables which keep the maximum amount of variation of the original dataset. Here, we plot the first two principal components and normalize the data for better visualization.</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">counts</span><span class="o">&lt;-</span><span class="n">scale</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span><span class="c1">##--- Cluster Visualization -------------------------------------------------</span><span class="w">
</span><span class="n">cluster.viz</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">doc.word.mat</span><span class="p">,</span><span class="w"> </span><span class="n">color.vector</span><span class="p">,</span><span class="w"> </span><span class="n">title</span><span class="o">=</span><span class="s1">' '</span><span class="p">){</span><span class="w">
  </span><span class="n">p.comp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prcomp</span><span class="p">(</span><span class="n">doc.word.mat</span><span class="p">,</span><span class="w"> </span><span class="n">scale.</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  </span><span class="n">plot</span><span class="p">(</span><span class="n">p.comp</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">color.vector</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w">  </span><span class="n">main</span><span class="o">=</span><span class="n">title</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">8</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="c1">## visualize the stimated clusters</span><span class="w">
</span><span class="n">counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span><span class="n">cluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">label.hat.hard</span><span class="p">,</span><span class="w"> </span><span class="s1">'Estimated Clusters (Hard EM)'</span><span class="p">)</span><span class="w">
</span><span class="n">cluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="s1">'True Labels'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><p><img src="/assets/images/EM_Hard_cluster.png" alt="png" /></p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">8</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="c1">## visualize the stimated clusters</span><span class="w">
</span><span class="n">counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span><span class="n">cluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">label.hat.soft</span><span class="p">,</span><span class="w"> </span><span class="s1">'Estimated Clusters (Soft EM)'</span><span class="p">)</span><span class="w">
</span><span class="n">textcluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="s1">'True Labels'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><p><img src="/assets/images/EM_soft_cluster.png" alt="png" /></p> <h2 id="now-judge-for-yourself">Now judge for yourself</h2> <p>Are these good clusters? Let’s have a look in a few examples and see if it makes sense.</p><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library(wordcloud)
library(RColorBrewer)
library(wordcloud2)

# Import data again
data &lt;- read.data(file.name='data\\bbc-text.csv', spr.ratio= .99)
counts &lt;- data$word.doc.mat
colnames(counts) &lt;- label.hat.hard
EM_cluster &lt;- t(rowsum(t(counts), group = colnames(counts), na.rm = T))
colnames(EM_cluster) &lt;- c("C1","C2","C3","C4")


# remove stopwords
stopwords &lt;- read.table('data\\stopwords.txt', header = FALSE, sep = "", dec = ".")
EM_cluster &lt;- as.data.frame(EM_cluster)
`%notin%` &lt;- Negate(`%in%`)
EM_cluster &lt;- subset(EM_cluster, rownames(EM_cluster) %notin% as.vector(stopwords[,1]))


# Generate wordclouds
options(repr.plot.width=15, repr.plot.height=10)

par(mfrow=c(2,2))

W1 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C1, min.freq = 0,
          max.words=1000, random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"))

W2 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C2, min.freq = 0,
          max.words=1000, random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"))

W3 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C3, min.freq = 0,
          max.words=1000, random.order=FALSE,
          colors=brewer.pal(8, "Dark2"))

W4 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C4, min.freq = 0,
          max.words=100, random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"))
</code></pre></div></div><p><img src="/assets/images/wordcloud.png" alt="png" /></p> </article></div><hr class="dashed" /><div class="container"><div class="row" data-aos="fade-up"><div class="col-md-12"><div class="share-box"> <i class="fa fa-share-alt" aria-hidden="true"></i> <a class="f nostyle" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/blog/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-facebook-official fa"></i ></a> <a class="t nostyle" href="https://twitter.com/intent/tweet?text=&url=http://localhost:4000/blog/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-twitter fa"></i ></a> <a class="g nostyle" href="https://plus.google.com/share?url=http://localhost:4000/blog/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i ></a> <a class="r nostyle" href="http://www.reddit.com/submit?url=http://localhost:4000/blog/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-reddit fa"></i ></a> <a class="l nostyle" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/blog/EM-Clustering/&title=&summary=&source=webjeda" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-linkedin fa"></i ></a> <a class="e nostyle" href="mailto:?subject=&amp;body=Check&amp;out&amp;this&amp;site&amp;http://localhost:4000/blog/EM-Clustering/" ><i class="fa fa-envelope fa"></i ></a></div></div></div><div class="row"><div class="col-md-12"> <p class="categories" data-aos="fade-up"> <span><a href="/categories/#clustering">Clustering</a></span> <span><a href="/categories/#projects">Projects</a></span> </p></div></div><div id="disqus_thread" data-aos="fade-up"></div><script defer> (function() { var d = document, s = d.createElement("script"); s.src = "//webjeda-demo.disqus.com/embed.js"; s.setAttribute("data-timestamp", +new Date()); (d.head || d.body).appendChild(s); })();</script><noscript>Please enable JavaScript to view the comments</noscript><div class="recent" data-aos="fade-up"><div class=""> <h3>Recent Articles</h3></div><div class="recent-grid"><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/Recommender-System/"><div class="cards"><div class="image" style="background-image: url(/assets/images/recommendation.JPG)" ></div><p class="title"><small>Recommender system using Pyspark (ALS algorithm)</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/Bootstrap-to-Quantify-Uncertainty/"><div class="cards"><div class="image" style="background-image: url(/assets/images/bootstrap_inf.png)" ></div><p class="title"><small>Bootstrap to Quantify Uncertainty</small></p></div></a></div></div></div></div></main><footer data-aos="fade-up"><div class="text-right"> <p class="copy"> <i class="fa fa-at"></i> 2018 <a class="rev" href="http://localhost:4000">Blackcurrant</a> </p></div></footer></div></div></div></div><!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1"> <style> .sidenav { width: 230px; position: fixed; z-index: 1; top: 100px; left: 30px; overflow-x: hidden; padding: 8px 0; } .sidenav a { padding: 6px 8px 6px 16px; text-decoration: none; color: #444; font-size: 22px; display: block; } .sidenav a:hover { color: #064579; } .top-bar { background-color: #222222; } @media only screen and (min-width: 800px) { #test { display: none; } } </style></head><body><div class="sidenav"> <style> #content-desktop { display: inline; } #content-mobile { display: none; } </style><div class="d-none d-sm-block"> <a href="http://www.alangewerc.com"><i class="fa fa-fw fa-home"></i> Home</a> <a href="http://www.alangewerc.com/projects"><i class="fa fa-area-chart"></i> Projects</a> <a href="http://www.alangewerc.com/blog"><i class="fa fa-book"></i> Articles</a> <a href="http://www.alangewerc.com/contact"><i class="fa fa-address-book-o"></i> Contact</a></div></div><script src="/assets/js/jQuery.min.js"></script><script> $(document).ready(function () { $(".loader").hide(); });</script><script> (function () { var css = document.createElement('link'); css.href = '/assets/font-awesome-4.7.0/css/font-awesome.min.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="/assets/font-awesome-4.7.0/css/font-awesome.min.css"> </noscript><script> $("#search-input").keyup(function () { $("main").hide(); $("search-container").show(); if (!$('#search-input').val()) { $("main").show(); $("search-container").hide(); } });</script><script src="/assets/js/jekyll-search.min.js" type="text/javascript"></script><script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-container'), searchResultTemplate: '<a class="nostyle" href="{url}"><div class="blog borders cards"><div class="image" style="background-image: url({image});"></div><div class="content"><h3 class="title">{title}</h3><p class="description">{description}</p></div></div></a>', noResultsText: 'No results found', json: '/search.json' })</script><script> (function () { var css = document.createElement('link'); css.href = 'https://unpkg.com/aos@2.3.1/dist/aos.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="/assets/animate-on-scroll/aos.min.css"> </noscript><script src="/assets/animate-on-scroll/aos.min.js"></script><script> AOS.init({ duration: 600, once: true, disable: 'mobile' });</script></body></html>