<!-- This layout is used in all pages. Making changes here will efect all pages. We recommend not to change anything here. --> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /><link rel="dns-prefetch" href="//fonts.googleapis.com" /><link rel="dns-prefetch" href="//google-analytics.com" /><link rel="dns-prefetch" href="//www.google-analytics.com" /><link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com" /><link rel="dns-prefetch" href="//ajax.googleapis.com" /><link rel="dns-prefetch" href="//fonts.gstatic.com" /><title>Expectation Maximization for News Clustering | Blackcurrant</title><meta name="generator" content="Jekyll v3.8.4" /><meta property="og:title" content="Expectation Maximization for News Clustering" /><meta name="author" content="Alan Gewerc" /><meta property="og:locale" content="en_US" /><meta name="description" content="Image source: Sklearn Introduction to Clustering Data clustering is the most commom form of unsupervised learning. The primary goal of unsupervised learning is to detect patterns in unlabeled data, which means there are no previous targets to forecasted. Clustering algorithms are useful for several tasks such as anomaly detection, customer segmentation, Insurance Fraud Detection and so on. Some of the most popular clustering algorithms developed are Center-based partitioning (e.g., KMeans), density-based clustering (e.g., DBSCAN), hierarchical clustering, and graphbased clustering. From geeksforgeeks Clustering is very much important as it determines the intrinsic grouping among the unlabeled data present. There are no criteria for a good clustering. It depends on the user, what is the criteria they may use which satisfy their need. For instance, we could be interested in finding representatives for homogeneous groups (data reduction), in finding “natural clusters” and describe their unknown properties (“natural” data types), in finding useful and suitable groupings (“useful” data classes) or in finding unusual data objects (outlier detection). This algorithm must make some assumptions which constitute the similarity of points and each assumption make different and equally valid clusters.” Here is an intuitive example of how clustering works: From this image we can understand how clustering works. We will choose different attributes from our data points. In this image family, place, gender are used to create clusters. Other possibilities could be considered. We will always make numerical manipulations of our points according to the features we have from the data. Expectation Maximization for Clustering The approach we will follow for EM in this project follows the work developed by professor Gholamreza Haffari from Monash University. In this project we will cluster news from the BBC dataset. There are 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport or tech. However, we are not interested in the labels as we will be applying an unsupervised technique. We will use the Expectation Maximization algorithm. From rapidminer The EM (expectation maximization) technique is similar to the K-Means technique. The basic operation of K-Means clustering algorithms is relatively simple: Given a fixed number of k clusters, assign observations to those clusters so that the means across clusters (for all variables) are as different from each other as possible. The EM algorithm extends this basic approach to clustering in two important ways: Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters. The basic approach and logic of this clustering method is as follows. Suppose you measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. The goal of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). Put another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. The results of EM clustering are different from those computed by k-means clustering. The latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. In other words, each observation belongs to each cluster with a certain probability. Of course, as a final result you can usually review an actual assignment of observations to clusters, based on the (largest) classification probability. An image from the Ardian Umam blog This is 3D Visualization of different clusters. As we can see they follow a gaussian distribution. We want to maximize the likelihood of points belonging to clusters . Project Development Programming Language: R 3.6.1 in Jupyter Notebook Libraries: reshape2 ggplot2 repr NLP tm Import Libraries options(warn=-1) library(reshape2) # for melt and cast functions library(ggplot2) # for plotting functions library(repr) # to resize the plots library(NLP) # natural language preprocessing library(tm) # text mining library Expectation Maximization Math for Document Clustering We firt derive the Expectation and Maximization steps of the hard-EM algorithm for Document Clustering: In Expectation and Maximization steps we have incomplete data, i.e., the documents clusters are not given to us so the latent variables are unseen. Let’s be the collection of parameters where the parameters means: is the clusters proportion, with and (1) is the word proportion for each cluster, with and (2) Our likelihood, the probability of the observed documents is: Applying the log in the equation above it yields: And the $\mathcal{Q}$ function (that is the basis of the EM algorithm) takes the form of: where are the responsability factors. Maximizing the $\mathcal{Q}$ function using the Lagrangian to enforce the constraints (1) and (2) above, and setting the derivatives to zero leads to the following solutions for the parameters: where (3) being the cluster proportion (4) being the word proportion for each cluster Thus, the EM Algorithm to learn the parameters and find the best values for the latent variables will follow these steps: 1) Choose an initial setting for the parameters $\theta^{old} := (\varphi^{old},\mu_1^{old},…,\mu_K^{old})$ In the case of hard-EM algorithm, each data is assignment to one class with the largest posterior probability. Thus: And there is no expectation in over the latent variables in the definition of the $\mathcal{Q}$ function. Thus: 2) While the convergence (stop condition) is not met: Expectation (E) step: based on the current values for the parameters $\theta^{old} := (\varphi^{old},\mu_1^{old},…,\mu_K^{old})$ we will set $\forall n$ and $\forall k$ a $Z^*$ such as: Maximization (M) step: based on the result of $ Z^{*}$ calculated on the E-step above, we re-estimate the values of parameters $(\varphi_k,\mu_{k,w})$ to calculate the $\theta^{new}$ using the equations (3) and (4) above: what setting the partial derivatives to zero leads to the following solutions for the parameters: where –&gt; being the cluster proportion –&gt; being the word proportion for each cluster 3) Implementation of the Hard-EM (derived above) and soft-EM for document clustering. Helper Function This function is needed to prevent numerical overflow/underflow when working with small numbers, because we can easily get small numbers by multiplying . Example: Suppose we are interested in where all numbers are probabilities in . To prevent numerical errors, we do the computation in the log space and convert the result back using the exp func. Hence our approach is to form the vector Then get the results by: Input: Output: logSum &lt;- function(v) { m = max(v) return ( m + log(sum(exp(v-m)))) } Initialize model parameters randomly initial.param &lt;- function(vocab_size, K=4, seed=123456){ set.seed(seed) rho &lt;- matrix(1/K,nrow = K, ncol=1) # assume all clusters have the same size (we will update this later on) mu &lt;- matrix(runif(K*vocab_size),nrow = K, ncol = vocab_size) # initiate Mu mu &lt;- prop.table(mu, margin = 1) # normalization to ensure that sum of each row is 1 return (list(&quot;rho&quot; = rho, &quot;mu&quot;= mu)) } E-Step for Document Clustering We now use a function to perform the E-step iteraction. Some information about the parameters of the function: Inputs: gamma: the matrix of posterior probabilities NxK model: the list of parameters (priors Kx1 and means KxVocabSize) counts: the word-document frequency matrix soft: TRUE if soft EM, FALSE if Hard EM Outputs: gamma: the matrix of posterior probabilities NxK for the given parameters (if soft = TRUE) z_star: the vector Nx1 of hard choosen classes for each document given gamma (if soft = FALSE) E.step &lt;- function(gamma, model, counts, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents K &lt;- dim(model$mu)[1] # E step: for (n in 1:N){ for (k in 1:K){ ## calculate the posterior based on the estimated mu and rho in the &quot;log space&quot; gamma[n,k] &lt;- log(model$rho[k,1]) + sum(counts[,n] * log(model$mu[k,])) } # normalisation to sum to 1 in the log space logZ = logSum(gamma[n,]) gamma[n,] = gamma[n,] - logZ } # converting back from the log space gamma &lt;- exp(gamma) # if it is hard EM, we need to select the K with highest gamma if(soft == FALSE){ z_star = max.col(gamma, &#39;first&#39;) # gets the &quot;argmax&quot; class for each observation } # if we are doing Hard-EM, we return &quot;z_star&quot;, which are the classes assigned for each n, # if we are doing soft-EM, we return &quot;gamma&quot;, which is the matrix with posterior probabilities for each k,n if(soft==FALSE){ return(z_star) }else{return(gamma)} } M Step for Document Clustering Inputs: gamma.z_star (if soft = TRUE) : the matrix of posterior probabilities NxK for the given parameters gamma.z_star (if soft = FALSE) : the vector Nx1 of hard choosen classes for each document given gamma model: the list of old parameters (class priors Kx1 and class means KxVocabSize) counts: the word-document frequency matrix soft: TRUE if soft EM, FALSE if Hard EM Outputs: model: the list of updated parameters (priors Kx1 and means KxVocabSize) M.step &lt;- function(gamma.z_star, model, counts, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents W &lt;- dim(counts)[1] # number of words i.e. vocabulary size K &lt;- dim(model$mu)[1] # number of clusters # to avoid NaN where all elements are zeros when calculating the new means eps = matrix(1e-10, nrow = W, ncol = N) if(soft== TRUE){ # Soft-EM gamma = gamma.z_star for (k in 1:K){ ## recalculate the estimations: model$rho[k] &lt;- sum(gamma[,k])/N # the relative cluster size model$mu[k,] &lt;- ((counts%*%gamma[,k])+eps[,k])/sum((counts%*%gamma[,k])+eps[,k]) # new means } }else{ # Hard-EM z_star = gamma.z_star for (k in 1:K){ ## recalculate the estimations: ## recalculate the estimations: model$rho[k] &lt;- sum(z_star==k)/N # the relative cluster size model$mu[k,] &lt;- rowSums(counts[,z_star==k]+eps[,z_star==k])/sum(rowSums(counts[,z_star==k]+eps[,z_star==k])) # new means } } # Return the result return (model) } The Training Objective Function Inputs: model: the model object containing the mu and rho counts: the word-document frequency matrix Outputs: nloglike: the negative log-likelihood i.e. log P(counts / model) train_obj &lt;- function(model, counts) { N &lt;- dim(counts)[2] # number of documents K &lt;- dim(model$mu)[1] nloglike = 0 for (n in 1:N){ lprob &lt;- matrix(0,ncol = 1, nrow=K) for (k in 1:K){ lprob[k,1] = sum(counts[,n] * log(model$mu[k,])) } nloglike &lt;- nloglike - logSum(lprob + log(model$rho)) } return (nloglike) } EM for Document Clustering Inputs: counts: word count matrix K: the number of clusters Outputs: model: a list of model parameters EM &lt;- function(counts, K=4, max.epoch=10, seed=123456, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents W &lt;- dim(counts)[1] # number of unique words (in all documents) # Initialization model &lt;- initial.param(W, K=K, seed=seed) gamma &lt;- matrix(0, nrow = N, ncol = K) print(train_obj(model,counts)) # Build the model for(epoch in 1:max.epoch){ # E Step gamma_kmax &lt;- E.step(gamma, model, counts, soft = soft) # M Step model &lt;- M.step(gamma_kmax, model, counts, soft = soft) print(train_obj(model, counts)) } # Return Model if(soft==TRUE){ return(list(&quot;model&quot;=model,&quot;gamma&quot;=gamma_kmax))}else{ return(list(&quot;model&quot;=model,&quot;k_max&quot;=gamma_kmax)) } } Read and Pre-process Data Inputs: file.name: name of the input .txt file spr.ratio: is used to reduce the sparcity of data by removing very infrequent words Outputs: docs: the unlabled corpus (each row is a document) word.doc.mat: the count matrix (each rows and columns corresponds to words and documents, respectively) label: the real cluster labels (will be used in visualization/validation and not for clustering) # reading the data read.data &lt;- function(file.name=&#39;./bbc-text.csv&#39;, spr.ratio=0.90) { # Read the data text_df &lt;- read.csv(file.name, colClasses = c(&#39;factor&#39;, &#39;character&#39;)) ## the terms before the first &#39;\t&#39; are the lables (the newsgroup names) and all the remaining text after &#39;\t&#39; ##are the actual documents docs &lt;- text_df colnames(docs) &lt;- c(&quot;doc_id&quot;, &quot;text&quot;) docs$doc_id &lt;- rownames(docs) # store the labels for evaluation labels &lt;- text_df$category library(tm) # create a corpus docs &lt;- DataframeSource(docs) corp &lt;- Corpus(docs) # Preprocessing: corp &lt;- tm_map(corp, removeWords, stopwords(&quot;english&quot;)) # remove stop words #(the most common word in a language that can be find in any document) corp &lt;- tm_map(corp, removePunctuation) # remove pnctuation corp &lt;- tm_map(corp, stemDocument) # perform stemming (reducing inflected and derived words to their root form) corp &lt;- tm_map(corp, removeNumbers) # remove all numbers corp &lt;- tm_map(corp, stripWhitespace) # remove redundant spaces # Create a matrix which its rows are the documents and colomns are the words. dtm &lt;- DocumentTermMatrix(corp) ## reduce the sparcity of out dtm dtm &lt;- removeSparseTerms(dtm, spr.ratio) ## convert dtm to a matrix word.doc.mat &lt;- t(as.matrix(dtm)) # Return the result return (list(&quot;docs&quot; = docs, &quot;word.doc.mat&quot;= word.doc.mat, &quot;labels&quot; = labels)) } # Reading documents ## Note: sample.size=0 means all read all documents! data &lt;- read.data(file.name=&#39;./bbc-text.csv&#39;, spr.ratio= .99) # word-document frequency matrix counts &lt;- data$word.doc.mat head(counts) Word Doc1 Doc2 Doc3 Doc4 … accord 1 0 0 0 … adam 1 0 0 0 … advert 1 0 0 0 … advertis 2 0 0 0 … allow 2 0 0 0 … alreadi 1 1 0 1 … also 3 0 0 0 … although 1 0 0 0 … announc 1 0 0 0 … annual 1 0 0 0 … anybodi 1 0 1 0 … avail 1 0 0 0 … bbc 3 0 1 0 … big 1 0 2 0 … biggest 1 0 0 0 … bill 1 0 0 0 … book 1 2 0 0 … box 2 0 0 0 … brand 7 0 0 0 … … 2 0 0 0 … # run soft-EM algorithm on the provided data res_soft &lt;- EM(counts, K=5, max.epoch=10, seed = 200) # visualization ## find the culster with the maximum probability (since we have soft assignment here) label.hat.soft &lt;- apply(res_soft$gamma, 1, which.max) [1] 3049919 [1] 2713476 [1] 2667142 [1] 2645417 [1] 2635238 [1] 2628601 [1] 2624423 [1] 2622443 [1] 2621742 [1] 2621387 [1] 2621310 # run soft-EM algorithm on the provided data res_hard &lt;- EM(counts, K=4, max.epoch=10, soft = FALSE, seed = 200) # visualization ## find the choosen cluster (since we have hard assignment here) label.hat.hard &lt;- res_hard$k_max [1] 3053548 [1] 2720130 [1] 2686664 [1] 2670998 [1] 2657195 [1] 2651093 [1] 2649465 [1] 2648918 [1] 2648824 [1] 2648763 [1] 2648748 Visualizing Clusters With PCA We normalize the count matrix for better visualization. counts&lt;-scale(counts) ##--- Cluster Visualization ------------------------------------------------- cluster.viz &lt;- function(doc.word.mat, color.vector, title=&#39; &#39;){ p.comp &lt;- prcomp(doc.word.mat, scale. = TRUE, center = TRUE) plot(p.comp$x, col=color.vector, pch=1, main=title) } options(repr.plot.width=15, repr.plot.height=8) par(mfrow=c(1,2)) ## visualize the stimated clusters counts &lt;- scale(counts) cluster.viz(t(counts), label.hat.hard, &#39;Estimated Clusters (Hard EM)&#39;) cluster.viz(t(counts), data$labels, &#39;True Labels&#39;) options(repr.plot.width=15, repr.plot.height=8) par(mfrow=c(1,2)) ## visualize the stimated clusters counts &lt;- scale(counts) cluster.viz(t(counts), label.hat.soft, &#39;Estimated Clusters (Soft EM)&#39;) textcluster.viz(t(counts), data$labels, &#39;True Labels&#39;) Now judge for yourself Are these good clusters? Lets have a look in a few examples and see if it makes sense. library(wordcloud) library(RColorBrewer) library(wordcloud2) # Import data again data &lt;- read.data(file.name=&#39;data\\bbc-text.csv&#39;, spr.ratio= .99) counts &lt;- data$word.doc.mat colnames(counts) &lt;- label.hat.hard EM_cluster &lt;- t(rowsum(t(counts), group = colnames(counts), na.rm = T)) colnames(EM_cluster) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) # remove stopwords stopwords &lt;- read.table(&#39;data\\stopwords.txt&#39;, header = FALSE, sep = &quot;&quot;, dec = &quot;.&quot;) EM_cluster &lt;- as.data.frame(EM_cluster) `%notin%` &lt;- Negate(`%in%`) EM_cluster &lt;- subset(EM_cluster, rownames(EM_cluster) %notin% as.vector(stopwords[,1])) # Generate wordclouds options(repr.plot.width=15, repr.plot.height=10) par(mfrow=c(2,2)) W1 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C1, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W2 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C2, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W3 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C3, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W4 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C4, min.freq = 0, max.words=100, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;))" /><meta property="og:description" content="Image source: Sklearn Introduction to Clustering Data clustering is the most commom form of unsupervised learning. The primary goal of unsupervised learning is to detect patterns in unlabeled data, which means there are no previous targets to forecasted. Clustering algorithms are useful for several tasks such as anomaly detection, customer segmentation, Insurance Fraud Detection and so on. Some of the most popular clustering algorithms developed are Center-based partitioning (e.g., KMeans), density-based clustering (e.g., DBSCAN), hierarchical clustering, and graphbased clustering. From geeksforgeeks Clustering is very much important as it determines the intrinsic grouping among the unlabeled data present. There are no criteria for a good clustering. It depends on the user, what is the criteria they may use which satisfy their need. For instance, we could be interested in finding representatives for homogeneous groups (data reduction), in finding “natural clusters” and describe their unknown properties (“natural” data types), in finding useful and suitable groupings (“useful” data classes) or in finding unusual data objects (outlier detection). This algorithm must make some assumptions which constitute the similarity of points and each assumption make different and equally valid clusters.” Here is an intuitive example of how clustering works: From this image we can understand how clustering works. We will choose different attributes from our data points. In this image family, place, gender are used to create clusters. Other possibilities could be considered. We will always make numerical manipulations of our points according to the features we have from the data. Expectation Maximization for Clustering The approach we will follow for EM in this project follows the work developed by professor Gholamreza Haffari from Monash University. In this project we will cluster news from the BBC dataset. There are 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport or tech. However, we are not interested in the labels as we will be applying an unsupervised technique. We will use the Expectation Maximization algorithm. From rapidminer The EM (expectation maximization) technique is similar to the K-Means technique. The basic operation of K-Means clustering algorithms is relatively simple: Given a fixed number of k clusters, assign observations to those clusters so that the means across clusters (for all variables) are as different from each other as possible. The EM algorithm extends this basic approach to clustering in two important ways: Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters. The basic approach and logic of this clustering method is as follows. Suppose you measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. The goal of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). Put another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. The results of EM clustering are different from those computed by k-means clustering. The latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. In other words, each observation belongs to each cluster with a certain probability. Of course, as a final result you can usually review an actual assignment of observations to clusters, based on the (largest) classification probability. An image from the Ardian Umam blog This is 3D Visualization of different clusters. As we can see they follow a gaussian distribution. We want to maximize the likelihood of points belonging to clusters . Project Development Programming Language: R 3.6.1 in Jupyter Notebook Libraries: reshape2 ggplot2 repr NLP tm Import Libraries options(warn=-1) library(reshape2) # for melt and cast functions library(ggplot2) # for plotting functions library(repr) # to resize the plots library(NLP) # natural language preprocessing library(tm) # text mining library Expectation Maximization Math for Document Clustering We firt derive the Expectation and Maximization steps of the hard-EM algorithm for Document Clustering: In Expectation and Maximization steps we have incomplete data, i.e., the documents clusters are not given to us so the latent variables are unseen. Let’s be the collection of parameters where the parameters means: is the clusters proportion, with and (1) is the word proportion for each cluster, with and (2) Our likelihood, the probability of the observed documents is: Applying the log in the equation above it yields: And the $\mathcal{Q}$ function (that is the basis of the EM algorithm) takes the form of: where are the responsability factors. Maximizing the $\mathcal{Q}$ function using the Lagrangian to enforce the constraints (1) and (2) above, and setting the derivatives to zero leads to the following solutions for the parameters: where (3) being the cluster proportion (4) being the word proportion for each cluster Thus, the EM Algorithm to learn the parameters and find the best values for the latent variables will follow these steps: 1) Choose an initial setting for the parameters $\theta^{old} := (\varphi^{old},\mu_1^{old},…,\mu_K^{old})$ In the case of hard-EM algorithm, each data is assignment to one class with the largest posterior probability. Thus: And there is no expectation in over the latent variables in the definition of the $\mathcal{Q}$ function. Thus: 2) While the convergence (stop condition) is not met: Expectation (E) step: based on the current values for the parameters $\theta^{old} := (\varphi^{old},\mu_1^{old},…,\mu_K^{old})$ we will set $\forall n$ and $\forall k$ a $Z^*$ such as: Maximization (M) step: based on the result of $ Z^{*}$ calculated on the E-step above, we re-estimate the values of parameters $(\varphi_k,\mu_{k,w})$ to calculate the $\theta^{new}$ using the equations (3) and (4) above: what setting the partial derivatives to zero leads to the following solutions for the parameters: where –&gt; being the cluster proportion –&gt; being the word proportion for each cluster 3) Implementation of the Hard-EM (derived above) and soft-EM for document clustering. Helper Function This function is needed to prevent numerical overflow/underflow when working with small numbers, because we can easily get small numbers by multiplying . Example: Suppose we are interested in where all numbers are probabilities in . To prevent numerical errors, we do the computation in the log space and convert the result back using the exp func. Hence our approach is to form the vector Then get the results by: Input: Output: logSum &lt;- function(v) { m = max(v) return ( m + log(sum(exp(v-m)))) } Initialize model parameters randomly initial.param &lt;- function(vocab_size, K=4, seed=123456){ set.seed(seed) rho &lt;- matrix(1/K,nrow = K, ncol=1) # assume all clusters have the same size (we will update this later on) mu &lt;- matrix(runif(K*vocab_size),nrow = K, ncol = vocab_size) # initiate Mu mu &lt;- prop.table(mu, margin = 1) # normalization to ensure that sum of each row is 1 return (list(&quot;rho&quot; = rho, &quot;mu&quot;= mu)) } E-Step for Document Clustering We now use a function to perform the E-step iteraction. Some information about the parameters of the function: Inputs: gamma: the matrix of posterior probabilities NxK model: the list of parameters (priors Kx1 and means KxVocabSize) counts: the word-document frequency matrix soft: TRUE if soft EM, FALSE if Hard EM Outputs: gamma: the matrix of posterior probabilities NxK for the given parameters (if soft = TRUE) z_star: the vector Nx1 of hard choosen classes for each document given gamma (if soft = FALSE) E.step &lt;- function(gamma, model, counts, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents K &lt;- dim(model$mu)[1] # E step: for (n in 1:N){ for (k in 1:K){ ## calculate the posterior based on the estimated mu and rho in the &quot;log space&quot; gamma[n,k] &lt;- log(model$rho[k,1]) + sum(counts[,n] * log(model$mu[k,])) } # normalisation to sum to 1 in the log space logZ = logSum(gamma[n,]) gamma[n,] = gamma[n,] - logZ } # converting back from the log space gamma &lt;- exp(gamma) # if it is hard EM, we need to select the K with highest gamma if(soft == FALSE){ z_star = max.col(gamma, &#39;first&#39;) # gets the &quot;argmax&quot; class for each observation } # if we are doing Hard-EM, we return &quot;z_star&quot;, which are the classes assigned for each n, # if we are doing soft-EM, we return &quot;gamma&quot;, which is the matrix with posterior probabilities for each k,n if(soft==FALSE){ return(z_star) }else{return(gamma)} } M Step for Document Clustering Inputs: gamma.z_star (if soft = TRUE) : the matrix of posterior probabilities NxK for the given parameters gamma.z_star (if soft = FALSE) : the vector Nx1 of hard choosen classes for each document given gamma model: the list of old parameters (class priors Kx1 and class means KxVocabSize) counts: the word-document frequency matrix soft: TRUE if soft EM, FALSE if Hard EM Outputs: model: the list of updated parameters (priors Kx1 and means KxVocabSize) M.step &lt;- function(gamma.z_star, model, counts, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents W &lt;- dim(counts)[1] # number of words i.e. vocabulary size K &lt;- dim(model$mu)[1] # number of clusters # to avoid NaN where all elements are zeros when calculating the new means eps = matrix(1e-10, nrow = W, ncol = N) if(soft== TRUE){ # Soft-EM gamma = gamma.z_star for (k in 1:K){ ## recalculate the estimations: model$rho[k] &lt;- sum(gamma[,k])/N # the relative cluster size model$mu[k,] &lt;- ((counts%*%gamma[,k])+eps[,k])/sum((counts%*%gamma[,k])+eps[,k]) # new means } }else{ # Hard-EM z_star = gamma.z_star for (k in 1:K){ ## recalculate the estimations: ## recalculate the estimations: model$rho[k] &lt;- sum(z_star==k)/N # the relative cluster size model$mu[k,] &lt;- rowSums(counts[,z_star==k]+eps[,z_star==k])/sum(rowSums(counts[,z_star==k]+eps[,z_star==k])) # new means } } # Return the result return (model) } The Training Objective Function Inputs: model: the model object containing the mu and rho counts: the word-document frequency matrix Outputs: nloglike: the negative log-likelihood i.e. log P(counts / model) train_obj &lt;- function(model, counts) { N &lt;- dim(counts)[2] # number of documents K &lt;- dim(model$mu)[1] nloglike = 0 for (n in 1:N){ lprob &lt;- matrix(0,ncol = 1, nrow=K) for (k in 1:K){ lprob[k,1] = sum(counts[,n] * log(model$mu[k,])) } nloglike &lt;- nloglike - logSum(lprob + log(model$rho)) } return (nloglike) } EM for Document Clustering Inputs: counts: word count matrix K: the number of clusters Outputs: model: a list of model parameters EM &lt;- function(counts, K=4, max.epoch=10, seed=123456, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents W &lt;- dim(counts)[1] # number of unique words (in all documents) # Initialization model &lt;- initial.param(W, K=K, seed=seed) gamma &lt;- matrix(0, nrow = N, ncol = K) print(train_obj(model,counts)) # Build the model for(epoch in 1:max.epoch){ # E Step gamma_kmax &lt;- E.step(gamma, model, counts, soft = soft) # M Step model &lt;- M.step(gamma_kmax, model, counts, soft = soft) print(train_obj(model, counts)) } # Return Model if(soft==TRUE){ return(list(&quot;model&quot;=model,&quot;gamma&quot;=gamma_kmax))}else{ return(list(&quot;model&quot;=model,&quot;k_max&quot;=gamma_kmax)) } } Read and Pre-process Data Inputs: file.name: name of the input .txt file spr.ratio: is used to reduce the sparcity of data by removing very infrequent words Outputs: docs: the unlabled corpus (each row is a document) word.doc.mat: the count matrix (each rows and columns corresponds to words and documents, respectively) label: the real cluster labels (will be used in visualization/validation and not for clustering) # reading the data read.data &lt;- function(file.name=&#39;./bbc-text.csv&#39;, spr.ratio=0.90) { # Read the data text_df &lt;- read.csv(file.name, colClasses = c(&#39;factor&#39;, &#39;character&#39;)) ## the terms before the first &#39;\t&#39; are the lables (the newsgroup names) and all the remaining text after &#39;\t&#39; ##are the actual documents docs &lt;- text_df colnames(docs) &lt;- c(&quot;doc_id&quot;, &quot;text&quot;) docs$doc_id &lt;- rownames(docs) # store the labels for evaluation labels &lt;- text_df$category library(tm) # create a corpus docs &lt;- DataframeSource(docs) corp &lt;- Corpus(docs) # Preprocessing: corp &lt;- tm_map(corp, removeWords, stopwords(&quot;english&quot;)) # remove stop words #(the most common word in a language that can be find in any document) corp &lt;- tm_map(corp, removePunctuation) # remove pnctuation corp &lt;- tm_map(corp, stemDocument) # perform stemming (reducing inflected and derived words to their root form) corp &lt;- tm_map(corp, removeNumbers) # remove all numbers corp &lt;- tm_map(corp, stripWhitespace) # remove redundant spaces # Create a matrix which its rows are the documents and colomns are the words. dtm &lt;- DocumentTermMatrix(corp) ## reduce the sparcity of out dtm dtm &lt;- removeSparseTerms(dtm, spr.ratio) ## convert dtm to a matrix word.doc.mat &lt;- t(as.matrix(dtm)) # Return the result return (list(&quot;docs&quot; = docs, &quot;word.doc.mat&quot;= word.doc.mat, &quot;labels&quot; = labels)) } # Reading documents ## Note: sample.size=0 means all read all documents! data &lt;- read.data(file.name=&#39;./bbc-text.csv&#39;, spr.ratio= .99) # word-document frequency matrix counts &lt;- data$word.doc.mat head(counts) Word Doc1 Doc2 Doc3 Doc4 … accord 1 0 0 0 … adam 1 0 0 0 … advert 1 0 0 0 … advertis 2 0 0 0 … allow 2 0 0 0 … alreadi 1 1 0 1 … also 3 0 0 0 … although 1 0 0 0 … announc 1 0 0 0 … annual 1 0 0 0 … anybodi 1 0 1 0 … avail 1 0 0 0 … bbc 3 0 1 0 … big 1 0 2 0 … biggest 1 0 0 0 … bill 1 0 0 0 … book 1 2 0 0 … box 2 0 0 0 … brand 7 0 0 0 … … 2 0 0 0 … # run soft-EM algorithm on the provided data res_soft &lt;- EM(counts, K=5, max.epoch=10, seed = 200) # visualization ## find the culster with the maximum probability (since we have soft assignment here) label.hat.soft &lt;- apply(res_soft$gamma, 1, which.max) [1] 3049919 [1] 2713476 [1] 2667142 [1] 2645417 [1] 2635238 [1] 2628601 [1] 2624423 [1] 2622443 [1] 2621742 [1] 2621387 [1] 2621310 # run soft-EM algorithm on the provided data res_hard &lt;- EM(counts, K=4, max.epoch=10, soft = FALSE, seed = 200) # visualization ## find the choosen cluster (since we have hard assignment here) label.hat.hard &lt;- res_hard$k_max [1] 3053548 [1] 2720130 [1] 2686664 [1] 2670998 [1] 2657195 [1] 2651093 [1] 2649465 [1] 2648918 [1] 2648824 [1] 2648763 [1] 2648748 Visualizing Clusters With PCA We normalize the count matrix for better visualization. counts&lt;-scale(counts) ##--- Cluster Visualization ------------------------------------------------- cluster.viz &lt;- function(doc.word.mat, color.vector, title=&#39; &#39;){ p.comp &lt;- prcomp(doc.word.mat, scale. = TRUE, center = TRUE) plot(p.comp$x, col=color.vector, pch=1, main=title) } options(repr.plot.width=15, repr.plot.height=8) par(mfrow=c(1,2)) ## visualize the stimated clusters counts &lt;- scale(counts) cluster.viz(t(counts), label.hat.hard, &#39;Estimated Clusters (Hard EM)&#39;) cluster.viz(t(counts), data$labels, &#39;True Labels&#39;) options(repr.plot.width=15, repr.plot.height=8) par(mfrow=c(1,2)) ## visualize the stimated clusters counts &lt;- scale(counts) cluster.viz(t(counts), label.hat.soft, &#39;Estimated Clusters (Soft EM)&#39;) textcluster.viz(t(counts), data$labels, &#39;True Labels&#39;) Now judge for yourself Are these good clusters? Lets have a look in a few examples and see if it makes sense. library(wordcloud) library(RColorBrewer) library(wordcloud2) # Import data again data &lt;- read.data(file.name=&#39;data\\bbc-text.csv&#39;, spr.ratio= .99) counts &lt;- data$word.doc.mat colnames(counts) &lt;- label.hat.hard EM_cluster &lt;- t(rowsum(t(counts), group = colnames(counts), na.rm = T)) colnames(EM_cluster) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) # remove stopwords stopwords &lt;- read.table(&#39;data\\stopwords.txt&#39;, header = FALSE, sep = &quot;&quot;, dec = &quot;.&quot;) EM_cluster &lt;- as.data.frame(EM_cluster) `%notin%` &lt;- Negate(`%in%`) EM_cluster &lt;- subset(EM_cluster, rownames(EM_cluster) %notin% as.vector(stopwords[,1])) # Generate wordclouds options(repr.plot.width=15, repr.plot.height=10) par(mfrow=c(2,2)) W1 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C1, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W2 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C2, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W3 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C3, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W4 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C4, min.freq = 0, max.words=100, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;))" /><link rel="canonical" href="http://localhost:4000/projects/EM-Clustering/" /><meta property="og:url" content="http://localhost:4000/projects/EM-Clustering/" /><meta property="og:site_name" content="Blackcurrant" /><meta property="og:image" content="http://localhost:4000/assets/images/cluster.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2018-11-01T00:00:00+11:00" /><script type="application/ld+json"> {"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/EM-Clustering/"},"datePublished":"2018-11-01T00:00:00+11:00","description":"Image source: Sklearn Introduction to Clustering Data clustering is the most commom form of unsupervised learning. The primary goal of unsupervised learning is to detect patterns in unlabeled data, which means there are no previous targets to forecasted. Clustering algorithms are useful for several tasks such as anomaly detection, customer segmentation, Insurance Fraud Detection and so on. Some of the most popular clustering algorithms developed are Center-based partitioning (e.g., KMeans), density-based clustering (e.g., DBSCAN), hierarchical clustering, and graphbased clustering. From geeksforgeeks Clustering is very much important as it determines the intrinsic grouping among the unlabeled data present. There are no criteria for a good clustering. It depends on the user, what is the criteria they may use which satisfy their need. For instance, we could be interested in finding representatives for homogeneous groups (data reduction), in finding “natural clusters” and describe their unknown properties (“natural” data types), in finding useful and suitable groupings (“useful” data classes) or in finding unusual data objects (outlier detection). This algorithm must make some assumptions which constitute the similarity of points and each assumption make different and equally valid clusters.” Here is an intuitive example of how clustering works: From this image we can understand how clustering works. We will choose different attributes from our data points. In this image family, place, gender are used to create clusters. Other possibilities could be considered. We will always make numerical manipulations of our points according to the features we have from the data. Expectation Maximization for Clustering The approach we will follow for EM in this project follows the work developed by professor Gholamreza Haffari from Monash University. In this project we will cluster news from the BBC dataset. There are 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport or tech. However, we are not interested in the labels as we will be applying an unsupervised technique. We will use the Expectation Maximization algorithm. From rapidminer The EM (expectation maximization) technique is similar to the K-Means technique. The basic operation of K-Means clustering algorithms is relatively simple: Given a fixed number of k clusters, assign observations to those clusters so that the means across clusters (for all variables) are as different from each other as possible. The EM algorithm extends this basic approach to clustering in two important ways: Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters. The basic approach and logic of this clustering method is as follows. Suppose you measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. The goal of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). Put another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. The results of EM clustering are different from those computed by k-means clustering. The latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. In other words, each observation belongs to each cluster with a certain probability. Of course, as a final result you can usually review an actual assignment of observations to clusters, based on the (largest) classification probability. An image from the Ardian Umam blog This is 3D Visualization of different clusters. As we can see they follow a gaussian distribution. We want to maximize the likelihood of points belonging to clusters . Project Development Programming Language: R 3.6.1 in Jupyter Notebook Libraries: reshape2 ggplot2 repr NLP tm Import Libraries options(warn=-1) library(reshape2) # for melt and cast functions library(ggplot2) # for plotting functions library(repr) # to resize the plots library(NLP) # natural language preprocessing library(tm) # text mining library Expectation Maximization Math for Document Clustering We firt derive the Expectation and Maximization steps of the hard-EM algorithm for Document Clustering: In Expectation and Maximization steps we have incomplete data, i.e., the documents clusters are not given to us so the latent variables are unseen. Let’s be the collection of parameters where the parameters means: is the clusters proportion, with and (1) is the word proportion for each cluster, with and (2) Our likelihood, the probability of the observed documents is: Applying the log in the equation above it yields: And the $\\mathcal{Q}$ function (that is the basis of the EM algorithm) takes the form of: where are the responsability factors. Maximizing the $\\mathcal{Q}$ function using the Lagrangian to enforce the constraints (1) and (2) above, and setting the derivatives to zero leads to the following solutions for the parameters: where (3) being the cluster proportion (4) being the word proportion for each cluster Thus, the EM Algorithm to learn the parameters and find the best values for the latent variables will follow these steps: 1) Choose an initial setting for the parameters $\\theta^{old} := (\\varphi^{old},\\mu_1^{old},…,\\mu_K^{old})$ In the case of hard-EM algorithm, each data is assignment to one class with the largest posterior probability. Thus: And there is no expectation in over the latent variables in the definition of the $\\mathcal{Q}$ function. Thus: 2) While the convergence (stop condition) is not met: Expectation (E) step: based on the current values for the parameters $\\theta^{old} := (\\varphi^{old},\\mu_1^{old},…,\\mu_K^{old})$ we will set $\\forall n$ and $\\forall k$ a $Z^*$ such as: Maximization (M) step: based on the result of $ Z^{*}$ calculated on the E-step above, we re-estimate the values of parameters $(\\varphi_k,\\mu_{k,w})$ to calculate the $\\theta^{new}$ using the equations (3) and (4) above: what setting the partial derivatives to zero leads to the following solutions for the parameters: where –&gt; being the cluster proportion –&gt; being the word proportion for each cluster 3) Implementation of the Hard-EM (derived above) and soft-EM for document clustering. Helper Function This function is needed to prevent numerical overflow/underflow when working with small numbers, because we can easily get small numbers by multiplying . Example: Suppose we are interested in where all numbers are probabilities in . To prevent numerical errors, we do the computation in the log space and convert the result back using the exp func. Hence our approach is to form the vector Then get the results by: Input: Output: logSum &lt;- function(v) { m = max(v) return ( m + log(sum(exp(v-m)))) } Initialize model parameters randomly initial.param &lt;- function(vocab_size, K=4, seed=123456){ set.seed(seed) rho &lt;- matrix(1/K,nrow = K, ncol=1) # assume all clusters have the same size (we will update this later on) mu &lt;- matrix(runif(K*vocab_size),nrow = K, ncol = vocab_size) # initiate Mu mu &lt;- prop.table(mu, margin = 1) # normalization to ensure that sum of each row is 1 return (list(&quot;rho&quot; = rho, &quot;mu&quot;= mu)) } E-Step for Document Clustering We now use a function to perform the E-step iteraction. Some information about the parameters of the function: Inputs: gamma: the matrix of posterior probabilities NxK model: the list of parameters (priors Kx1 and means KxVocabSize) counts: the word-document frequency matrix soft: TRUE if soft EM, FALSE if Hard EM Outputs: gamma: the matrix of posterior probabilities NxK for the given parameters (if soft = TRUE) z_star: the vector Nx1 of hard choosen classes for each document given gamma (if soft = FALSE) E.step &lt;- function(gamma, model, counts, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents K &lt;- dim(model$mu)[1] # E step: for (n in 1:N){ for (k in 1:K){ ## calculate the posterior based on the estimated mu and rho in the &quot;log space&quot; gamma[n,k] &lt;- log(model$rho[k,1]) + sum(counts[,n] * log(model$mu[k,])) } # normalisation to sum to 1 in the log space logZ = logSum(gamma[n,]) gamma[n,] = gamma[n,] - logZ } # converting back from the log space gamma &lt;- exp(gamma) # if it is hard EM, we need to select the K with highest gamma if(soft == FALSE){ z_star = max.col(gamma, &#39;first&#39;) # gets the &quot;argmax&quot; class for each observation } # if we are doing Hard-EM, we return &quot;z_star&quot;, which are the classes assigned for each n, # if we are doing soft-EM, we return &quot;gamma&quot;, which is the matrix with posterior probabilities for each k,n if(soft==FALSE){ return(z_star) }else{return(gamma)} } M Step for Document Clustering Inputs: gamma.z_star (if soft = TRUE) : the matrix of posterior probabilities NxK for the given parameters gamma.z_star (if soft = FALSE) : the vector Nx1 of hard choosen classes for each document given gamma model: the list of old parameters (class priors Kx1 and class means KxVocabSize) counts: the word-document frequency matrix soft: TRUE if soft EM, FALSE if Hard EM Outputs: model: the list of updated parameters (priors Kx1 and means KxVocabSize) M.step &lt;- function(gamma.z_star, model, counts, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents W &lt;- dim(counts)[1] # number of words i.e. vocabulary size K &lt;- dim(model$mu)[1] # number of clusters # to avoid NaN where all elements are zeros when calculating the new means eps = matrix(1e-10, nrow = W, ncol = N) if(soft== TRUE){ # Soft-EM gamma = gamma.z_star for (k in 1:K){ ## recalculate the estimations: model$rho[k] &lt;- sum(gamma[,k])/N # the relative cluster size model$mu[k,] &lt;- ((counts%*%gamma[,k])+eps[,k])/sum((counts%*%gamma[,k])+eps[,k]) # new means } }else{ # Hard-EM z_star = gamma.z_star for (k in 1:K){ ## recalculate the estimations: ## recalculate the estimations: model$rho[k] &lt;- sum(z_star==k)/N # the relative cluster size model$mu[k,] &lt;- rowSums(counts[,z_star==k]+eps[,z_star==k])/sum(rowSums(counts[,z_star==k]+eps[,z_star==k])) # new means } } # Return the result return (model) } The Training Objective Function Inputs: model: the model object containing the mu and rho counts: the word-document frequency matrix Outputs: nloglike: the negative log-likelihood i.e. log P(counts / model) train_obj &lt;- function(model, counts) { N &lt;- dim(counts)[2] # number of documents K &lt;- dim(model$mu)[1] nloglike = 0 for (n in 1:N){ lprob &lt;- matrix(0,ncol = 1, nrow=K) for (k in 1:K){ lprob[k,1] = sum(counts[,n] * log(model$mu[k,])) } nloglike &lt;- nloglike - logSum(lprob + log(model$rho)) } return (nloglike) } EM for Document Clustering Inputs: counts: word count matrix K: the number of clusters Outputs: model: a list of model parameters EM &lt;- function(counts, K=4, max.epoch=10, seed=123456, soft = TRUE){ # Model Parameter Setting N &lt;- dim(counts)[2] # number of documents W &lt;- dim(counts)[1] # number of unique words (in all documents) # Initialization model &lt;- initial.param(W, K=K, seed=seed) gamma &lt;- matrix(0, nrow = N, ncol = K) print(train_obj(model,counts)) # Build the model for(epoch in 1:max.epoch){ # E Step gamma_kmax &lt;- E.step(gamma, model, counts, soft = soft) # M Step model &lt;- M.step(gamma_kmax, model, counts, soft = soft) print(train_obj(model, counts)) } # Return Model if(soft==TRUE){ return(list(&quot;model&quot;=model,&quot;gamma&quot;=gamma_kmax))}else{ return(list(&quot;model&quot;=model,&quot;k_max&quot;=gamma_kmax)) } } Read and Pre-process Data Inputs: file.name: name of the input .txt file spr.ratio: is used to reduce the sparcity of data by removing very infrequent words Outputs: docs: the unlabled corpus (each row is a document) word.doc.mat: the count matrix (each rows and columns corresponds to words and documents, respectively) label: the real cluster labels (will be used in visualization/validation and not for clustering) # reading the data read.data &lt;- function(file.name=&#39;./bbc-text.csv&#39;, spr.ratio=0.90) { # Read the data text_df &lt;- read.csv(file.name, colClasses = c(&#39;factor&#39;, &#39;character&#39;)) ## the terms before the first &#39;\\t&#39; are the lables (the newsgroup names) and all the remaining text after &#39;\\t&#39; ##are the actual documents docs &lt;- text_df colnames(docs) &lt;- c(&quot;doc_id&quot;, &quot;text&quot;) docs$doc_id &lt;- rownames(docs) # store the labels for evaluation labels &lt;- text_df$category library(tm) # create a corpus docs &lt;- DataframeSource(docs) corp &lt;- Corpus(docs) # Preprocessing: corp &lt;- tm_map(corp, removeWords, stopwords(&quot;english&quot;)) # remove stop words #(the most common word in a language that can be find in any document) corp &lt;- tm_map(corp, removePunctuation) # remove pnctuation corp &lt;- tm_map(corp, stemDocument) # perform stemming (reducing inflected and derived words to their root form) corp &lt;- tm_map(corp, removeNumbers) # remove all numbers corp &lt;- tm_map(corp, stripWhitespace) # remove redundant spaces # Create a matrix which its rows are the documents and colomns are the words. dtm &lt;- DocumentTermMatrix(corp) ## reduce the sparcity of out dtm dtm &lt;- removeSparseTerms(dtm, spr.ratio) ## convert dtm to a matrix word.doc.mat &lt;- t(as.matrix(dtm)) # Return the result return (list(&quot;docs&quot; = docs, &quot;word.doc.mat&quot;= word.doc.mat, &quot;labels&quot; = labels)) } # Reading documents ## Note: sample.size=0 means all read all documents! data &lt;- read.data(file.name=&#39;./bbc-text.csv&#39;, spr.ratio= .99) # word-document frequency matrix counts &lt;- data$word.doc.mat head(counts) Word Doc1 Doc2 Doc3 Doc4 … accord 1 0 0 0 … adam 1 0 0 0 … advert 1 0 0 0 … advertis 2 0 0 0 … allow 2 0 0 0 … alreadi 1 1 0 1 … also 3 0 0 0 … although 1 0 0 0 … announc 1 0 0 0 … annual 1 0 0 0 … anybodi 1 0 1 0 … avail 1 0 0 0 … bbc 3 0 1 0 … big 1 0 2 0 … biggest 1 0 0 0 … bill 1 0 0 0 … book 1 2 0 0 … box 2 0 0 0 … brand 7 0 0 0 … … 2 0 0 0 … # run soft-EM algorithm on the provided data res_soft &lt;- EM(counts, K=5, max.epoch=10, seed = 200) # visualization ## find the culster with the maximum probability (since we have soft assignment here) label.hat.soft &lt;- apply(res_soft$gamma, 1, which.max) [1] 3049919 [1] 2713476 [1] 2667142 [1] 2645417 [1] 2635238 [1] 2628601 [1] 2624423 [1] 2622443 [1] 2621742 [1] 2621387 [1] 2621310 # run soft-EM algorithm on the provided data res_hard &lt;- EM(counts, K=4, max.epoch=10, soft = FALSE, seed = 200) # visualization ## find the choosen cluster (since we have hard assignment here) label.hat.hard &lt;- res_hard$k_max [1] 3053548 [1] 2720130 [1] 2686664 [1] 2670998 [1] 2657195 [1] 2651093 [1] 2649465 [1] 2648918 [1] 2648824 [1] 2648763 [1] 2648748 Visualizing Clusters With PCA We normalize the count matrix for better visualization. counts&lt;-scale(counts) ##--- Cluster Visualization ------------------------------------------------- cluster.viz &lt;- function(doc.word.mat, color.vector, title=&#39; &#39;){ p.comp &lt;- prcomp(doc.word.mat, scale. = TRUE, center = TRUE) plot(p.comp$x, col=color.vector, pch=1, main=title) } options(repr.plot.width=15, repr.plot.height=8) par(mfrow=c(1,2)) ## visualize the stimated clusters counts &lt;- scale(counts) cluster.viz(t(counts), label.hat.hard, &#39;Estimated Clusters (Hard EM)&#39;) cluster.viz(t(counts), data$labels, &#39;True Labels&#39;) options(repr.plot.width=15, repr.plot.height=8) par(mfrow=c(1,2)) ## visualize the stimated clusters counts &lt;- scale(counts) cluster.viz(t(counts), label.hat.soft, &#39;Estimated Clusters (Soft EM)&#39;) textcluster.viz(t(counts), data$labels, &#39;True Labels&#39;) Now judge for yourself Are these good clusters? Lets have a look in a few examples and see if it makes sense. library(wordcloud) library(RColorBrewer) library(wordcloud2) # Import data again data &lt;- read.data(file.name=&#39;data\\\\bbc-text.csv&#39;, spr.ratio= .99) counts &lt;- data$word.doc.mat colnames(counts) &lt;- label.hat.hard EM_cluster &lt;- t(rowsum(t(counts), group = colnames(counts), na.rm = T)) colnames(EM_cluster) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) # remove stopwords stopwords &lt;- read.table(&#39;data\\\\stopwords.txt&#39;, header = FALSE, sep = &quot;&quot;, dec = &quot;.&quot;) EM_cluster &lt;- as.data.frame(EM_cluster) `%notin%` &lt;- Negate(`%in%`) EM_cluster &lt;- subset(EM_cluster, rownames(EM_cluster) %notin% as.vector(stopwords[,1])) # Generate wordclouds options(repr.plot.width=15, repr.plot.height=10) par(mfrow=c(2,2)) W1 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C1, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W2 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C2, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W3 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C3, min.freq = 0, max.words=1000, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) W4 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C4, min.freq = 0, max.words=100, random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;))","image":"http://localhost:4000/assets/images/cluster.png","author":{"@type":"Person","name":"Alan Gewerc"},"headline":"Expectation Maximization for News Clustering","url":"http://localhost:4000/projects/EM-Clustering/","dateModified":"2018-11-01T00:00:00+11:00","@context":"https://schema.org"}</script><link rel="stylesheet" href="/assets/css/main-default.css" /><link id="color-scheme" rel="stylesheet" href="/assets/css/main-default.css" /><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/apple-icon-60x60.png" /><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/apple-icon-114x114.png" /><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/apple-icon-152x152.png" /><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/android-icon-192x192.png" /><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" /><link rel="icon" href="/favicon.ico" type="image/x-icon" /><script src="https://cdn.jsdelivr.net/npm/ga-lite@1/dist/ga-lite.min.js" async></script><script> var galite = galite || {}; galite.UA = 'UA-92266803-3';</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script></head><div class="wrapper"><div class="container"><div class="main"><div class="main-container shadow"><div class="title-space"> <h1>Expectation Maximization for News Clustering</h1></div><hr class="dashed"> <main> <ul class="breadcrumbs"> <li><a href="/">Home</a></li> <li><a href="/projects/">Projects</a></li> <li><a href="#">Em clustering</a></li> </ul><div class="meta" data-aos="fade-up"> <p> <small> <span> <i class="fa fa-calendar" aria-hidden="true"></i> 01 Nov 2018&nbsp; </span> <span> <i class="fa fa-user" aria-hidden="true"></i> Alan Gewerc&nbsp; </span> <span> <i class="fa fa-clock-o" aria-hidden="true"></i> 27 mins read. </span> </small> </p></div><div class="featured-image" style="background-image: url(/assets/images/cluster.png)" data-aos="zoom-in" ></div><div class="container"> <article> <p><em>Image source: Sklearn</em></p> <p><br /></p> <h3 id="introduction-to-clustering">Introduction to Clustering</h3> <p>Data clustering is the most commom form of <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a>. The primary goal of unsupervised learning is to detect patterns in unlabeled data, which means there are no previous targets to forecasted. Clustering algorithms are useful for several tasks such as anomaly detection, customer segmentation, Insurance Fraud Detection and so on. Some of the most popular clustering algorithms developed are <a href="https://epubs.siam.org/doi/abs/10.1137/1.9780898718348.ch9?mobileUi=0">Center-based</a> partitioning (e.g., KMeans), <a href="https://en.wikipedia.org/wiki/Cluster_analysis#Density-based_clustering">density-based</a> clustering (e.g., DBSCAN), <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical</a> clustering, and <a href="http://www.slideshare.net/ssakpi/graph-based-clustering">graphbased</a> clustering. <br /></p> <p>From <a href="https://www.geeksforgeeks.org/clustering-in-machine-learning/">geeksforgeeks</a> <br /> <em>Clustering is very much important as it determines the intrinsic grouping among the unlabeled data present. There are no criteria for a good clustering. It depends on the user, what is the criteria they may use which satisfy their need. For instance, we could be interested in finding representatives for homogeneous groups (data reduction), in finding “natural clusters” and describe their unknown properties (“natural” data types), in finding useful and suitable groupings (“useful” data classes) or in finding unusual data objects (outlier detection). This algorithm must make some assumptions which constitute the similarity of points and each assumption make different and equally valid clusters.”</em></p> <p>Here is an intuitive example of how clustering works:</p> <p><img style="float: left;" src="/assets/images/simpsons.jpg" alt="drawing" width="600" height="500" /></p> <p><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p> <p>From this image we can understand how clustering works. We will choose different attributes from our data points. In this image family, place, gender are used to create clusters. Other possibilities could be considered. We will always make numerical manipulations of our points according to the features we have from the data. <br /></p> <h3 id="expectation-maximization-for-clustering-">Expectation Maximization for Clustering <br /></h3> <p>The approach we will follow for EM in this project follows the work developed by professor <a href="http://users.monash.edu.au/~gholamrh/">Gholamreza Haffari</a> from Monash University.</p> <p>In this project we will cluster news from the BBC dataset. There are 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport or tech. However, we are not interested in the labels as we will be applying an unsupervised technique. We will use the Expectation Maximization algorithm.</p> <p>From <a href="https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/expectation_maximization_clustering.html">rapidminer</a><br /> <i>The EM (expectation maximization) technique is similar to the K-Means technique. The basic operation of K-Means clustering algorithms is relatively simple: Given a fixed number of k clusters, assign observations to those clusters so that the means across clusters (for all variables) are as different from each other as possible. The EM algorithm extends this basic approach to clustering in two important ways: <br /></i></p> <ul> <li>Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters. <br /></li> <li>The basic approach and logic of this clustering method is as follows. Suppose you measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. The goal of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). Put another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. <br /></li> <li>The results of EM clustering are different from those computed by k-means clustering. The latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. In other words, each observation belongs to each cluster with a certain probability. Of course, as a final result you can usually review an actual assignment of observations to clusters, based on the (largest) classification probability.</li> </ul> <p><em>An image from the Ardian Umam</em> <a href="https://ardianumam.wordpress.com/2017/11/07/how-em-expectation-maximization-method-works-for-clustering/">blog</a><br /> <img src="/assets/images/em2.png" alt="drawing" width="500" height="300" /> <br /><br /><br /></p> <ul> <li><em>This is 3D Visualization of different clusters.</em></li> <li><em>As we can see they follow a gaussian distribution.</em></li> <li><em>We want to maximize the likelihood of points belonging to clusters .</em></li> </ul> <p><br /></p> <h2 id="project-development">Project Development</h2> <p>Programming Language: R 3.6.1 in Jupyter Notebook</p> <p>Libraries:</p> <ul> <li>reshape2</li> <li>ggplot2</li> <li>repr</li> <li>NLP</li> <li>tm</li> </ul> <h4 id="import-libraries">Import Libraries</h4><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">warn</span><span class="o">=</span><span class="m">-1</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">reshape2</span><span class="p">)</span><span class="w"> </span><span class="c1"># for melt and cast functions</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w"> </span><span class="c1"># for plotting functions</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">repr</span><span class="p">)</span><span class="w"> </span><span class="c1"># to resize the plots</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">NLP</span><span class="p">)</span><span class="w"> </span><span class="c1"># natural language preprocessing</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span><span class="w"> </span><span class="c1"># text mining library</span><span class="w">
</span></code></pre></div></div><p><br /><br /></p> <h2 id="expectation-maximization-math-for-document-clustering">Expectation Maximization Math for Document Clustering<br /></h2> <p>We firt derive the Expectation and Maximization steps of the <strong>hard-EM</strong> algorithm for Document Clustering:</p> <p>In <strong>Expectation</strong> and <strong>Maximization</strong> steps we have incomplete data, i.e., the documents clusters are not given to us so the latent variables<script type="math/tex">{z_1, z_2, ... , z_N}</script>are unseen.</p> <p>Let’s be<script type="math/tex">theta := (\varphi,\mu_1,...,\mu_K)</script>the collection of parameters where the parameters means:</p> <ul> <li><script type="math/tex">\varphi=(\varphi_1,\varphi_2,...,\varphi_K)</script>is the clusters proportion, with<script type="math/tex">\varphi_k \ge 0</script>and<script type="math/tex">\sum_{k=1}^{K} \varphi_k =1</script>(1)</li> <li><script type="math/tex">\mu_k=(\mu_{k,1},\mu_{k,2},...,\mu_{k,\|\mathcal{A}\| })</script>is the word proportion for each cluster, with<script type="math/tex">\mu_{k,w} \ge 0</script>and<script type="math/tex">\sum_{w \in \mathcal{A}} \mu_{k,w} =1</script>(2)</li> </ul> <p>Our likelihood, the probability of the observed documents is:<script type="math/tex">p(d_1,...d_N)=\prod _{n=1}^{N}p(d_n)=\prod _{n=1}^{N}\sum _{k=1}^{K}p(z_{n,k}=1,d_n)=\prod _{n=1}^{N}\sum _{k=1}^{K}\left ( \varphi _{k}\prod _{w\in\mathcal{A}}\mu_{k,w}^{c(w,d_n)} \right )</script></p> <p>Applying the log in the equation above it yields:<script type="math/tex">ln\ p(d_1,...d_N)=\sum _{n=1}^{N}ln\ p(d_n)=\sum _{n=1}^{N}ln\sum _{k=1}^{K}p(z_{n,k}=1,d_n)=\sum _{n=1}^{N}ln\sum _{k=1}^{K}\left ( \varphi _{k}\prod _{w\in\mathcal{A}}\mu_{k,w}^{c(w,d_n)} \right )</script></p> <p>And the $\mathcal{Q}$ function (that is the basis of the EM algorithm) takes the form of:<script type="math/tex">\mathcal{Q}\left ( \theta,\theta^{old} \right ):=\sum _{n=1}^{N}\sum _{k=1}^{K}p(z_{n,k}=1|d_n,\theta^{old})\ ln\ p(z_{n,k}=1,d_n|\theta)\\ \\=\sum _{n=1}^{N}\sum _{k=1}^{K}p(z_{n,k}=1|d_n,\theta^{old})\left ( ln\varphi _k + \sum _{w\in\mathcal{A}}{c(w,d_n)}\ ln\ \mu_{k,w}\right )\\ =\sum _{n=1}^{N}\sum _{k=1}^{K}\gamma (z_{n,k})\left ( ln\varphi _k + \sum _{w\in\mathcal{A}}{c(w,d_n)}\ ln\ \mu_{k,w}\right )\\</script></p> <p>where<script type="math/tex">\gamma (z_{n,k}):= p(z_{n,k}=1 \| d_n,\theta^{old})</script>are the responsability factors.</p> <p>Maximizing the $\mathcal{Q}$ function using the Lagrangian to enforce the constraints (1) and (2) above, and setting the derivatives to zero leads to the following solutions for the parameters:</p> <ul> <li><script type="math/tex">\varphi_k=\frac{N _k}{N}</script>where<script type="math/tex">N_k := \sum _{n=1}^{N}\gamma (z_{n,k})</script>(3) being the cluster proportion</li> <li><script type="math/tex">\mu_{k,w}=\frac{\sum _{n=1}^{N}\gamma (z_{n,k})c(w,d_n)}{\sum _{w'\in\mathcal{A}} \sum _{n=1}^{N}\gamma (z_{n,k})c(w',d_n)}</script>(4) being the word proportion for each cluster</li> </ul> <p>Thus, the EM Algorithm to learn the parameters and find the best values for the latent variables will follow these steps:</p> <p>1) Choose an initial setting for the parameters $\theta^{old} := (\varphi^{old},\mu_1^{old},…,\mu_K^{old})$</p> <p>In the case of <strong>hard-EM algorithm</strong>, each data is assignment to one class with the largest posterior probability. Thus:<br /><script type="math/tex">Z^{*} = argmax_z\ \gamma (z_{n,k})= argmax_z\ p(z_{n,k}=1|d_n,\theta^{old})</script></p> <p>And there is no expectation in over the latent variables in the definition of the $\mathcal{Q}$ function. Thus:<script type="math/tex">\mathcal{Q}(\theta,\theta^{old})= \sum _{n=1}^{N} ln\ p(z_{n,k=Z^*}=1,d_n|\theta)</script></p> <p>2) While the convergence (stop condition) is not met:</p> <ul> <li><strong>Expectation (E) step:</strong> based on the current values for the parameters $\theta^{old} := (\varphi^{old},\mu_1^{old},…,\mu_K^{old})$ we will set $\forall n$ and $\forall k$ a $Z^*$ such as:</li> </ul><script type="math/tex; mode=display">Z^{*} \leftarrow argmax_z\ \gamma (z_{n,k})= argmax_z\ p(z_{n,k}=1|d_n,\theta^{old})</script><ul> <li><strong>Maximization (M) step:</strong> based on the result of $ Z^{*}$ calculated on the <strong>E-step</strong> above, we re-estimate the values of parameters $(\varphi_k,\mu_{k,w})$ to calculate the $\theta^{new}$ using the equations (3) and (4) above:</li> </ul><script type="math/tex; mode=display">\theta^{new} \leftarrow argmax_\theta\ \mathcal{Q}(\theta,\theta^{old})= argmax_\theta \sum _{n=1}^{N} ln\ p(z_{n,k=Z^*}^{*}=1,d_n|\theta)\\ = argmax_\theta\ \sum _{n=1}^{N}\left (ln\varphi _{k=Z^*} + \sum _{w\in\mathcal{A}}{c(w,d_n)}ln\ \mu_{k=Z^*,w}\right )</script><p>what setting the partial derivatives to zero leads to the following solutions for the parameters:</p> <ul> <li><script type="math/tex">\varphi_k^{new}=\frac{N _k}{N}</script>where<script type="math/tex">N_k := \sum _{n=1}^{N}z_{n,k=Z^*}</script>–&gt; being the cluster proportion</li> <li><script type="math/tex">\mu_{k,w}^{new}=\frac{\sum _{n=1}^{N}z_{n,k=Z^*}c(w,d_n)}{\sum _{w'\in\mathcal{A}} \sum _{n=1}^{N} z_{n,k=Z^*}c(w',d_n)}</script>–&gt; being the word proportion for each cluster</li> </ul> <p>3)<script type="math/tex">\theta^{old}\leftarrow \theta^{new}</script></p> <p>Implementation of the Hard-EM (derived above) and soft-EM for document clustering.</p> <h4 id="helper-function">Helper Function</h4> <p>This function is needed to prevent numerical overflow/underflow when working with small numbers, because we can easily get small numbers by multiplying<script type="math/tex">% <![CDATA[ p1 * p2 * ... * pn (where 0 <= pi <= 1$ are probabilities) %]]></script>. Example: Suppose we are interested in<script type="math/tex">p1xp2xp3 + q1xq2xq3</script>where all numbers are probabilities in<script type="math/tex">[0,1]</script>. To prevent numerical errors, we do the computation in the log space and convert the result back using the exp func. Hence our approach is to form the vector<script type="math/tex">v = log(p1)+log(p2)+log(p3) , log(q1)+log(q2)+log(q3)]</script>Then get the results by:<script type="math/tex">exp(logSum(v))</script><br /> Input:<script type="math/tex">logA1, logA2 ... logAn</script><br /> Output:<script type="math/tex">log(A1+A2+...+An)</script></p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logSum</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
   </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w">
   </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">m</span><span class="p">))))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="initialize-model-parameters-randomly">Initialize model parameters randomly</h3><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial.param</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">123456</span><span class="p">){</span><span class="w">
  </span><span class="n">set.seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span><span class="w">
  </span><span class="n">rho</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="n">K</span><span class="p">,</span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="c1"># assume all clusters have the same size (we will update this later on)</span><span class="w">
  </span><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">runif</span><span class="p">(</span><span class="n">K</span><span class="o">*</span><span class="n">vocab_size</span><span class="p">),</span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vocab_size</span><span class="p">)</span><span class="w">    </span><span class="c1"># initiate Mu </span><span class="w">
  </span><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prop.table</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">margin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">   </span><span class="c1"># normalization to ensure that sum of each row is 1</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"rho"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rho</span><span class="p">,</span><span class="w"> </span><span class="s2">"mu"</span><span class="o">=</span><span class="w"> </span><span class="n">mu</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="e-step-for-document-clustering">E-Step for Document Clustering</h3> <p>We now use a function to perform the E-step iteraction. Some information about the parameters of the function:</p> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">gamma</code>: the matrix of posterior probabilities NxK</li> <li><code class="highlighter-rouge">model</code>: the list of parameters (priors Kx1 and means KxVocabSize)</li> <li><code class="highlighter-rouge">counts</code>: the word-document frequency matrix</li> <li><code class="highlighter-rouge">soft</code>: TRUE if soft EM, FALSE if Hard EM</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">gamma</code>: the matrix of posterior probabilities NxK for the given parameters (if soft = TRUE)</li> <li><code class="highlighter-rouge">z_star</code>: the vector Nx1 of hard choosen classes for each document given gamma (if soft = FALSE)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">E.step</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
  </span><span class="c1"># Model Parameter Setting</span><span class="w">
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">

  </span><span class="c1"># E step:    </span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">){</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
      </span><span class="c1">## calculate the posterior based on the estimated mu and rho in the "log space"</span><span class="w">
      </span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="m">1</span><span class="p">])</span><span class="w"> </span><span class="o">+</span><span class="w">  </span><span class="nf">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]))</span><span class="w"> 
    </span><span class="p">}</span><span class="w">
    </span><span class="c1"># normalisation to sum to 1 in the log space</span><span class="w">
    </span><span class="n">logZ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">logSum</span><span class="p">(</span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,])</span><span class="w">
    </span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma</span><span class="p">[</span><span class="n">n</span><span class="p">,]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">logZ</span><span class="w">
  </span><span class="p">}</span><span class="w">
  
  </span><span class="c1"># converting back from the log space </span><span class="w">
  </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span><span class="w">
  </span><span class="c1"># if it is hard EM, we need to select the K with highest gamma</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">){</span><span class="w">
      </span><span class="n">z_star</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max.col</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="s1">'first'</span><span class="p">)</span><span class="w"> </span><span class="c1"># gets the "argmax" class for each observation   </span><span class="w">
  </span><span class="p">}</span><span class="w">
  </span><span class="c1"># if we are doing Hard-EM, we return "z_star", which are the classes assigned for each n, </span><span class="w">
  </span><span class="c1"># if we are doing soft-EM, we return "gamma", which is the matrix with posterior probabilities for each k,n</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="o">==</span><span class="kc">FALSE</span><span class="p">){</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="n">z_star</span><span class="p">)</span><span class="w">
  </span><span class="p">}</span><span class="k">else</span><span class="p">{</span><span class="nf">return</span><span class="p">(</span><span class="n">gamma</span><span class="p">)}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="m-step-for-document-clustering">M Step for Document Clustering</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">gamma.z_star</code> (if soft = TRUE) : the matrix of posterior probabilities NxK for the given parameters</li> <li><code class="highlighter-rouge">gamma.z_star</code> (if soft = FALSE) : the vector Nx1 of hard choosen classes for each document given gamma</li> <li><code class="highlighter-rouge">model</code>: the list of old parameters (class priors Kx1 and class means KxVocabSize)</li> <li><code class="highlighter-rouge">counts</code>: the word-document frequency matrix</li> <li><code class="highlighter-rouge">soft</code>: TRUE if soft EM, FALSE if Hard EM</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">model</code>: the list of updated parameters (priors Kx1 and means KxVocabSize)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">M.step</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">gamma.z_star</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
  </span><span class="c1"># Model Parameter Setting</span><span class="w">
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w">   </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">W</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">   </span><span class="c1"># number of words i.e. vocabulary size</span><span class="w">
  </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of clusters</span><span class="w">
  </span><span class="c1"># to avoid NaN where all elements are zeros when calculating the new  means</span><span class="w">
  </span><span class="n">eps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">1e-10</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w">
  
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="o">==</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w"> </span><span class="c1"># Soft-EM</span><span class="w">
      </span><span class="n">gamma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma.z_star</span><span class="w">
      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
          </span><span class="c1">## recalculate the estimations:</span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="n">N</span><span class="w">  </span><span class="c1"># the relative cluster size</span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">((</span><span class="n">counts</span><span class="o">%*%</span><span class="n">gamma</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="nf">sum</span><span class="p">((</span><span class="n">counts</span><span class="o">%*%</span><span class="n">gamma</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">k</span><span class="p">])</span><span class="w"> </span><span class="c1"># new means</span><span class="w">
    
    </span><span class="p">}</span><span class="w">      
  </span><span class="p">}</span><span class="k">else</span><span class="p">{</span><span class="w"> </span><span class="c1"># Hard-EM</span><span class="w">
      </span><span class="n">z_star</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma.z_star</span><span class="w">
      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
          </span><span class="c1">## recalculate the estimations:</span><span class="w">
          </span><span class="c1">## recalculate the estimations:</span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="w">  </span><span class="c1"># the relative cluster size          </span><span class="w">
          </span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rowSums</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">rowSums</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">[,</span><span class="n">z_star</span><span class="o">==</span><span class="n">k</span><span class="p">]))</span><span class="w"> 
          </span><span class="c1"># new means</span><span class="w">
    </span><span class="p">}</span><span class="w">

  </span><span class="p">}</span><span class="w">

  </span><span class="c1"># Return the result</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="the-training-objective-function">The Training Objective Function</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">model</code>: the model object containing the mu and rho</li> <li><code class="highlighter-rouge">counts</code>: the word-document frequency matrix</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">nloglike</code>: the negative log-likelihood i.e. log P(counts / model)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> 
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">
   
  </span><span class="n">nloglike</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">){</span><span class="w">
    </span><span class="n">lprob</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="o">=</span><span class="n">K</span><span class="p">)</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">){</span><span class="w">
      </span><span class="n">lprob</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">[,</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,]))</span><span class="w"> 
    </span><span class="p">}</span><span class="w">
    </span><span class="n">nloglike</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nloglike</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">logSum</span><span class="p">(</span><span class="n">lprob</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">rho</span><span class="p">))</span><span class="w">
  </span><span class="p">}</span><span class="w">
  
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">nloglike</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="em-for-document-clustering">EM for Document Clustering</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">counts</code>: word count matrix</li> <li><code class="highlighter-rouge">K</code>: the number of clusters</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">model</code>: a list of model parameters</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EM</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">max.epoch</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">123456</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
  
  </span><span class="c1"># Model Parameter Setting</span><span class="w">
  </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of documents</span><span class="w">
  </span><span class="n">W</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">counts</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1"># number of unique words (in all documents)</span><span class="w">
  
  </span><span class="c1"># Initialization</span><span class="w">
  </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">initial.param</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span><span class="w">
  </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">)</span><span class="w">

  </span><span class="n">print</span><span class="p">(</span><span class="n">train_obj</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">counts</span><span class="p">))</span><span class="w">
  </span><span class="c1"># Build the model</span><span class="w">
  </span><span class="k">for</span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">max.epoch</span><span class="p">){</span><span class="w">
    
    </span><span class="c1"># E Step</span><span class="w">
    </span><span class="n">gamma_kmax</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">E.step</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">soft</span><span class="p">)</span><span class="w">
    </span><span class="c1"># M Step</span><span class="w">
    </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">M.step</span><span class="p">(</span><span class="n">gamma_kmax</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">soft</span><span class="p">)</span><span class="w">
   
    </span><span class="n">print</span><span class="p">(</span><span class="n">train_obj</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="p">))</span><span class="w"> 
  </span><span class="p">}</span><span class="w">
  </span><span class="c1"># Return Model</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">soft</span><span class="o">==</span><span class="kc">TRUE</span><span class="p">){</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"model"</span><span class="o">=</span><span class="n">model</span><span class="p">,</span><span class="s2">"gamma"</span><span class="o">=</span><span class="n">gamma_kmax</span><span class="p">))}</span><span class="k">else</span><span class="p">{</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"model"</span><span class="o">=</span><span class="n">model</span><span class="p">,</span><span class="s2">"k_max"</span><span class="o">=</span><span class="n">gamma_kmax</span><span class="p">))</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h3 id="read-and-pre-process-data">Read and Pre-process Data</h3> <p><strong>Inputs:</strong></p> <ul> <li><code class="highlighter-rouge">file.name</code>: name of the input .txt file</li> <li><code class="highlighter-rouge">spr.ratio</code>: is used to reduce the sparcity of data by removing very infrequent words</li> </ul> <p><strong>Outputs:</strong></p> <ul> <li><code class="highlighter-rouge">docs</code>: the unlabled corpus (each row is a document)</li> <li><code class="highlighter-rouge">word.doc.mat</code>: the count matrix (each rows and columns corresponds to words and documents, respectively)</li> <li><code class="highlighter-rouge">label</code>: the real cluster labels (will be used in visualization/validation and not for clustering)</li> </ul><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reading the data</span><span class="w">
</span><span class="n">read.data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">file.name</span><span class="o">=</span><span class="s1">'./bbc-text.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">spr.ratio</span><span class="o">=</span><span class="m">0.90</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">

  </span><span class="c1"># Read the data</span><span class="w">
  </span><span class="n">text_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="n">file.name</span><span class="p">,</span><span class="w"> </span><span class="n">colClasses</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'factor'</span><span class="p">,</span><span class="w"> </span><span class="s1">'character'</span><span class="p">))</span><span class="w">
  </span><span class="c1">## the terms before the first '\t' are the lables (the newsgroup names) and all the remaining text after '\t' </span><span class="w">
  </span><span class="c1">##are the actual documents</span><span class="w">
  </span><span class="n">docs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">text_df</span><span class="w">
  </span><span class="n">colnames</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"doc_id"</span><span class="p">,</span><span class="w"> </span><span class="s2">"text"</span><span class="p">)</span><span class="w">
  </span><span class="n">docs</span><span class="o">$</span><span class="n">doc_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rownames</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w">
  </span><span class="c1"># store the labels for evaluation</span><span class="w">
  </span><span class="n">labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">text_df</span><span class="o">$</span><span class="n">category</span><span class="w">
    
  </span><span class="n">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span><span class="w">
  </span><span class="c1"># create a corpus</span><span class="w">
  </span><span class="n">docs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">DataframeSource</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Corpus</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># Preprocessing:</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">removeWords</span><span class="p">,</span><span class="w"> </span><span class="n">stopwords</span><span class="p">(</span><span class="s2">"english"</span><span class="p">))</span><span class="w"> </span><span class="c1"># remove stop words </span><span class="w">
    </span><span class="c1">#(the most common word in a language that can be find in any document)</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">removePunctuation</span><span class="p">)</span><span class="w"> </span><span class="c1"># remove pnctuation</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">stemDocument</span><span class="p">)</span><span class="w"> </span><span class="c1"># perform stemming (reducing inflected and derived words to their root form)</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">removeNumbers</span><span class="p">)</span><span class="w"> </span><span class="c1"># remove all numbers</span><span class="w">
  </span><span class="n">corp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corp</span><span class="p">,</span><span class="w"> </span><span class="n">stripWhitespace</span><span class="p">)</span><span class="w"> </span><span class="c1"># remove redundant spaces </span><span class="w">
  </span><span class="c1"># Create a matrix which its rows are the documents and colomns are the words. </span><span class="w">
  </span><span class="n">dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">DocumentTermMatrix</span><span class="p">(</span><span class="n">corp</span><span class="p">)</span><span class="w">
  </span><span class="c1">## reduce the sparcity of out dtm</span><span class="w">
  </span><span class="n">dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">removeSparseTerms</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span><span class="w"> </span><span class="n">spr.ratio</span><span class="p">)</span><span class="w">
  </span><span class="c1">## convert dtm to a matrix</span><span class="w">
  </span><span class="n">word.doc.mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">dtm</span><span class="p">))</span><span class="w">
  
  </span><span class="c1"># Return the result</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="s2">"docs"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">docs</span><span class="p">,</span><span class="w"> </span><span class="s2">"word.doc.mat"</span><span class="o">=</span><span class="w"> </span><span class="n">word.doc.mat</span><span class="p">,</span><span class="w"> </span><span class="s2">"labels"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">labels</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1"># Reading documents </span><span class="w">
</span><span class="c1">## Note: sample.size=0 means all read all documents!</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.data</span><span class="p">(</span><span class="n">file.name</span><span class="o">=</span><span class="s1">'./bbc-text.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">spr.ratio</span><span class="o">=</span><span class="w"> </span><span class="m">.99</span><span class="p">)</span><span class="w">
</span><span class="c1"># word-document frequency matrix </span><span class="w">
</span><span class="n">counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">word.doc.mat</span><span class="w">
</span></code></pre></div></div><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><table> <thead> <tr> <th style="text-align: center"><strong>Word</strong></th> <th style="text-align: center"><strong>Doc1</strong></th> <th style="text-align: center"><strong>Doc2</strong></th> <th style="text-align: center"><strong>Doc3</strong></th> <th style="text-align: center"><strong>Doc4</strong></th> <th style="text-align: center"><strong>…</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">accord</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">adam</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">advert</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">advertis</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">allow</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">alreadi</td> <td style="text-align: center">1</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">1</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">also</td> <td style="text-align: center">3</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">although</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">announc</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">annual</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">anybodi</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">avail</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">bbc</td> <td style="text-align: center">3</td> <td style="text-align: center">0</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">big</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">biggest</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">bill</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">book</td> <td style="text-align: center">1</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">box</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">brand</td> <td style="text-align: center">7</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> <tr> <td style="text-align: center">…</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> <td style="text-align: center">…</td> </tr> </tbody> </table><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#  run soft-EM algorithm on the provided data</span><span class="w">
</span><span class="n">res_soft</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">EM</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">max.epoch</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200</span><span class="p">)</span><span class="w">  
</span><span class="c1"># visualization</span><span class="w">
</span><span class="c1">## find the culster with the maximum probability (since we have soft assignment here)</span><span class="w">
</span><span class="n">label.hat.soft</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">res_soft</span><span class="o">$</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">which.max</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 3049919
[1] 2713476
[1] 2667142
[1] 2645417
[1] 2635238
[1] 2628601
[1] 2624423
[1] 2622443
[1] 2621742
[1] 2621387
[1] 2621310
</code></pre></div></div><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#  run soft-EM algorithm on the provided data</span><span class="w">
</span><span class="n">res_hard</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">EM</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">max.epoch</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200</span><span class="p">)</span><span class="w">  
</span><span class="c1"># visualization</span><span class="w">
</span><span class="c1">## find the choosen cluster (since we have hard assignment here)</span><span class="w">
</span><span class="n">label.hat.hard</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">res_hard</span><span class="o">$</span><span class="n">k_max</span><span class="w">
</span></code></pre></div></div><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 3053548
[1] 2720130
[1] 2686664
[1] 2670998
[1] 2657195
[1] 2651093
[1] 2649465
[1] 2648918
[1] 2648824
[1] 2648763
[1] 2648748
</code></pre></div></div><h1 id="visualizing-clusters-with-pca">Visualizing Clusters With PCA</h1> <p>We normalize the count matrix for better visualization.</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">counts</span><span class="o">&lt;-</span><span class="n">scale</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span><span class="c1">##--- Cluster Visualization -------------------------------------------------</span><span class="w">
</span><span class="n">cluster.viz</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">doc.word.mat</span><span class="p">,</span><span class="w"> </span><span class="n">color.vector</span><span class="p">,</span><span class="w"> </span><span class="n">title</span><span class="o">=</span><span class="s1">' '</span><span class="p">){</span><span class="w">
  </span><span class="n">p.comp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prcomp</span><span class="p">(</span><span class="n">doc.word.mat</span><span class="p">,</span><span class="w"> </span><span class="n">scale.</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  </span><span class="n">plot</span><span class="p">(</span><span class="n">p.comp</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">color.vector</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w">  </span><span class="n">main</span><span class="o">=</span><span class="n">title</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">8</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="c1">## visualize the stimated clusters</span><span class="w">
</span><span class="n">counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span><span class="n">cluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">label.hat.hard</span><span class="p">,</span><span class="w"> </span><span class="s1">'Estimated Clusters (Hard EM)'</span><span class="p">)</span><span class="w">
</span><span class="n">cluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="s1">'True Labels'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><p><img src="/assets/images/EM_Hard_cluster.png" alt="png" /></p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">8</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="c1">## visualize the stimated clusters</span><span class="w">
</span><span class="n">counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="w">
</span><span class="n">cluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">label.hat.soft</span><span class="p">,</span><span class="w"> </span><span class="s1">'Estimated Clusters (Soft EM)'</span><span class="p">)</span><span class="w">
</span><span class="n">textcluster.viz</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="s1">'True Labels'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><p><img src="/assets/images/EM_soft_cluster.png" alt="png" /></p> <h2 id="now-judge-for-yourself">Now judge for yourself</h2> <p>Are these good clusters? Lets have a look in a few examples and see if it makes sense.</p><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library(wordcloud)
library(RColorBrewer)
library(wordcloud2)

# Import data again
data &lt;- read.data(file.name='data\\bbc-text.csv', spr.ratio= .99)
counts &lt;- data$word.doc.mat
colnames(counts) &lt;- label.hat.hard
EM_cluster &lt;- t(rowsum(t(counts), group = colnames(counts), na.rm = T))
colnames(EM_cluster) &lt;- c("C1","C2","C3","C4")


# remove stopwords
stopwords &lt;- read.table('data\\stopwords.txt', header = FALSE, sep = "", dec = ".")
EM_cluster &lt;- as.data.frame(EM_cluster)
`%notin%` &lt;- Negate(`%in%`)
EM_cluster &lt;- subset(EM_cluster, rownames(EM_cluster) %notin% as.vector(stopwords[,1]))


# Generate wordclouds
options(repr.plot.width=15, repr.plot.height=10)

par(mfrow=c(2,2))

W1 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C1, min.freq = 0,
          max.words=1000, random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"))

W2 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C2, min.freq = 0,
          max.words=1000, random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"))

W3 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C3, min.freq = 0,
          max.words=1000, random.order=FALSE,
          colors=brewer.pal(8, "Dark2"))

W4 &lt;- wordcloud(words = rownames(EM_cluster), freq = EM_cluster$C4, min.freq = 0,
          max.words=100, random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"))
</code></pre></div></div><p><img src="/assets/images/wordcloud.png" alt="png" /></p> </article></div><hr class="dashed" /><div class="container"><div class="row" data-aos="fade-up"><div class="col-md-12"><div class="share-box"> <i class="fa fa-share-alt" aria-hidden="true"></i> <a class="f nostyle" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/projects/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-facebook-official fa"></i ></a> <a class="t nostyle" href="https://twitter.com/intent/tweet?text=&url=http://localhost:4000/projects/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-twitter fa"></i ></a> <a class="g nostyle" href="https://plus.google.com/share?url=http://localhost:4000/projects/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i ></a> <a class="r nostyle" href="http://www.reddit.com/submit?url=http://localhost:4000/projects/EM-Clustering/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-reddit fa"></i ></a> <a class="l nostyle" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/projects/EM-Clustering/&title=&summary=&source=webjeda" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-linkedin fa"></i ></a> <a class="e nostyle" href="mailto:?subject=&amp;body=Check&amp;out&amp;this&amp;site&amp;http://localhost:4000/projects/EM-Clustering/" ><i class="fa fa-envelope fa"></i ></a></div></div></div><div class="row"><div class="col-md-12"> <p class="categories" data-aos="fade-up"> <span><a href="/categories/#clustering">Clustering</a></span> <span><a href="/categories/#projects">Projects</a></span> </p></div></div><div id="disqus_thread" data-aos="fade-up"></div><script defer> (function() { var d = document, s = d.createElement("script"); s.src = "//webjeda-demo.disqus.com/embed.js"; s.setAttribute("data-timestamp", +new Date()); (d.head || d.body).appendChild(s); })();</script><noscript>Please enable JavaScript to view the comments</noscript><div class="recent" data-aos="fade-up"><div class=""> <h3>Recent Articles</h3></div><div class="recent-grid"><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/dog/"><div class="cards"><div class="image" style="background-image: url(/assets/images/anthony-tori-102062.jpg)" ></div><p class="title"><small>An Algorithm for a Dog Identification App</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/blackcurrant/"><div class="cards"><div class="image" style="background-image: url(/assets/images/blackcurrant.jpg)" ></div><p class="title"><small>Blackcurrant Jekyll Theme</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/corporateCreditRatingPrediction/"><div class="cards"><div class="image" style="background-image: url(/assets/images/finance.jpg)" ></div><p class="title"><small>Corporate Credit Rating Forecasting</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/EM-Clustering/"><div class="cards"><div class="image" style="background-image: url(/assets/images/cluster.png)" ></div><p class="title"><small>Expectation Maximization for News Clustering</small></p></div></a></div></div></div></div></main><footer data-aos="fade-up"><div class="text-right"> <p class="copy"> <i class="fa fa-at"></i> 2018 <a class="rev" href="http://localhost:4000">Blackcurrant</a> </p></div></footer></div></div></div></div><!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1"> <style> .sidenav { width: 230px; position: fixed; z-index: 1; top: 100px; left: 30px; overflow-x: hidden; padding: 8px 0; } .sidenav a { padding: 6px 8px 6px 16px; text-decoration: none; color: #444; font-size: 22px; display: block; } .sidenav a:hover { color: #064579; } .top-bar { background-color: #222222; } </style></head><body><div class="sidenav"> <a href="#home"><i class="fa fa-fw fa-home"></i> Home</a> <a href="#Projects"><i class="fa fa-area-chart"></i> Projects</a></div><script src="/assets/js/jQuery.min.js"></script><script> $(document).ready(function () { $(".loader").hide(); });</script><script> (function () { var css = document.createElement('link'); css.href = '/assets/font-awesome-4.7.0/css/font-awesome.min.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="/assets/font-awesome-4.7.0/css/font-awesome.min.css"> </noscript><script> $("#search-input").keyup(function () { $("main").hide(); $("search-container").show(); if (!$('#search-input').val()) { $("main").show(); $("search-container").hide(); } });</script><script src="/assets/js/jekyll-search.min.js" type="text/javascript"></script><script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-container'), searchResultTemplate: '<a class="nostyle" href="{url}"><div class="blog borders cards"><div class="image" style="background-image: url({image});"></div><div class="content"><h3 class="title">{title}</h3><p class="description">{description}</p></div></div></a>', noResultsText: 'No results found', json: '/search.json' })</script><script> (function () { var css = document.createElement('link'); css.href = 'https://unpkg.com/aos@2.3.1/dist/aos.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="/assets/animate-on-scroll/aos.min.css"> </noscript><script src="/assets/animate-on-scroll/aos.min.js"></script><script> AOS.init({ duration: 600, once: true, disable: 'mobile' });</script></body></html>